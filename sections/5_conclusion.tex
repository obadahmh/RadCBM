This work introduced a novel framework for unsupervised deep neural network interpretability that aims to address the shortcomings of current concept bottleneck models (CBMs). Leveraging pre-trained models to extract semantically rich features, the method enforces interpretability using a $k$-sparse autoencoder ($k$-SAE), with sparsity as the sole regularization. Results demonstrate the proposed method compares favorably to state-of-the-art concept-based interpretable models in terms of predictive accuracy and interpretability. The proposed method also uniquely localizes learned concepts, revealing the spatial regions that contribute to the model's predictions. This localization gives insight into the understanding of the network's decision-making process and also confirms the semantic relevance of the extracted features.
% Future work
Future works include adapting this system to open-vocabulary concepts and fusing generative models to generate visual concepts from textual prompts. 