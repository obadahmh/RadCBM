% =============================================================================
% RadCBM Methods Section
% =============================================================================
\label{sec:method}

\subsection{Concept Bank Construction from Reports}
Let $\mathcal{D}=\{(x_i,r_i,y_i)\}_{i=1}^N$ denote a dataset of chest radiographs $x_i$, associated free-text reports $r_i$ (when available), and multi-label targets $y_i\in\{0,1,-1\}^L$ for $L$ clinical labels (CheXpert-style), where $-1$ denotes uncertainty or missingness.
\radcbm\ predicts labels through an explicit concept bottleneck: we distill a concept bank $\mathcal{C}=\{c_1,\dots,c_K\}$ from reports and train models that first predict concepts from images and then predict diagnoses from the predicted concepts, where $\hat{c}\in[0,1]^K$ is a vector of concept probabilities.

\paragraph{Entity extraction and assertion.}
We apply RadGraph-XL~\cite{jain2021radgraph} to report findings and impression sections to extract entity mentions and their assertion status (\emph{present}, \emph{absent}, \emph{uncertain}).
For each mention we also retain modifier spans linked by RadGraph relations, which often encode anatomical context such as laterality or coarse location.
These modifiers are normalized into a free-text location string that is carried forward as weak spatial context.

\paragraph{Ontology linking.}
Each extracted mention is linked to a UMLS Concept Unique Identifier (CUI) using SapBERT~\cite{liu2021sapbert} embeddings and nearest-neighbor retrieval over a synonym index built from UMLS.
To reduce off-domain matches, we restrict candidate synonyms to a curated set of radiology-relevant semantic types and constrain sources to SNOMED CT terms~\cite{bodenreider2004umls,donnelly2006snomed}.
We embed both the mention surface form and the mention concatenated with its modifier tokens, then keep the highest-scoring candidate above a similarity threshold (default 0.8), discarding low-confidence links.

\paragraph{Study-level aggregation and inventory.}
For each study, we aggregate linked mentions into a set of concept records containing the canonical concept name, linked CUI, assertion status, and derived location string.
If a study contains multiple mentions mapping to the same concept, we keep the highest-precedence assertion, where present overrides uncertain and uncertain overrides absent, to avoid contradictory supervision when forming study-level labels.
Aggregating across studies yields an inventory with per-concept frequencies, supported assertions, and observed locations.

\paragraph{Pruning and label tensors.}
To obtain a CBM-friendly vocabulary, we prune the inventory by concept category (findings by default) and frequency (for example, at least 10 total occurrences with at least one positive mention), with optional name-based filters to remove non-informative normality concepts.
We then create an ordered concept index and a dense study-by-concept label matrix using a multi-assertion encoding $a_{ik}\in\{0,1,2,3\}$ for (unmentioned, absent, uncertain, present).
This encoding supports \emph{mention-masked} supervision: unmentioned concepts are treated as unknown rather than negative.
Although our pipeline can retain anatomy scaffold and device concepts, we use finding concepts by default to form the bottleneck representation.
In the current pruned bank, this procedure yields $K=1{,}312$ ontology-grounded findings.

\paragraph{Optional MI-based filtering.}
When a more compact, task-specific concept set is desired, we optionally apply label-aware filtering using mutual information (MI) between concept presence and downstream labels.
We binarize concept presence using only \emph{present} assertions and compute $I(c_k;y_j)$ for each concept and label on the training set, then retain concepts that exceed a threshold or fall within the top K by $\max_j I(c_k;y_j)$.
This step is optional and is used to trade vocabulary size for label relevance.

\paragraph{Coarse anatomical regions.}
We define a coarse anatomical partition $\mathcal{R}$ consisting of lung, pleura, heart, mediastinum, bone, and an additional other bucket.
Each concept $k$ is assigned to a parent region $g(k)\in\mathcal{R}$ using deterministic rules based on supported location strings, simple name cues (for example, pleur, pulmon, mediastin), and semantic type.
This fixed concept to region map is shared across all hierarchical components.

Given any concept-valued vector $v\in[0,1]^K$, we define pooled region scores by max-pooling over concepts in each region:
\begin{equation}
\label{eq:region_pool}
P_r(v)=\max_{k:\,g(k)=r} v_k \quad \forall r\in\mathcal{R}.
\end{equation}
When report-derived concept supervision is available, we derive pooled region targets $\tilde{z}_{ir}=P_r(t_i)$ with a corresponding mask indicating whether any concept in region $r$ is explicitly mentioned; unmentioned regions are excluded from region losses.

\subsection{RadCBM Model and Training}
\radcbm\ follows a two-stage design: an image$\to$concept predictor trained from report-derived concept supervision, and a concept$\to$label head trained to map predicted concepts to downstream diagnoses.
Both stages can include region prediction and multiplicative gating through the shared concept to region map.

\paragraph{Data alignment and splits.}
All training and evaluation are performed at the study level.
We use predefined train, validation, and test splits and align (i) images, (ii) report-derived concept supervision, and (iii) disease-label CSV targets by study identifier to avoid leakage and to ensure that the diagnosis head is trained on the same population for which concept predictions are produced.

\paragraph{Concept supervision and uncertainty handling.}
From the multi-assertion matrix $a_{ik}$ we derive targets $t_{ik}\in\{0,0.5,1\}$ and a mention mask $m_{ik}\in\{0,1\}$.
Unmentioned concepts are masked ($m_{ik}=0$); absent and present are supervised as negative and positive; uncertain mentions are either ignored (masked) or treated as soft targets ($t_{ik}=0.5$) with a reduced loss weight.
For disease labels, we treat uncertain labels as missing by default and ignore them in the label loss and in per-class metrics, with optional mappings of uncertainty to positive or negative for sensitivity analyses.

\paragraph{Stage 1: image to concepts (and regions).}
A radiology-pretrained vision backbone maps the image to a feature vector $h=f_\theta(x)$, implemented with MedCLIP~\cite{wang2022medclip} (ViT or ResNet variants) or standard ResNet backbones.
A lightweight two-layer MLP produces concept logits $s=g_\phi(h)$ and probabilities $\hat{c}=\sigma(s)$.
We optimize a mention-masked binary cross-entropy that supervises only concepts that are explicitly asserted:
\begin{equation}
\label{eq:concept_loss}
\mathcal{L}_{\text{concept}}=
\frac{1}{\sum_{i,k} m_{ik}}\sum_{i,k} m_{ik}\;\mathrm{BCE}(s_{ik},t_{ik}).
\end{equation}

When a region map is available, we additionally learn a region head that predicts region logits from the intermediate concept activations.
We supervise region logits using pooled targets from Eq.~(\ref{eq:region_pool}) when available, and we optionally clamp predicted region gates using these pooled targets, treating unknown regions as neutral.
Region probabilities are used as multiplicative gates on concept activations, which discourages anatomically implausible concept predictions while preserving a soft failure mode controlled by the gate floor and temperature.
The concept predictor can further be regularized with an ontology-aware graph Laplacian penalty over concept head weights:
\begin{equation}
\label{eq:graph_reg}
\mathcal{L}_{\text{graph}}=\sum_{(p,q)\in E}\|w_p-w_q\|_2^2,
\end{equation}
where $E$ is a user-specified set of concept edges, including a simple within-region chain graph or externally provided edges.
When region supervision is enabled, the Stage 1 objective becomes $\mathcal{L}_{\text{concept}}+\lambda_r\mathcal{L}_{\text{region}}+\lambda_g\mathcal{L}_{\text{graph}}$, where $\mathcal{L}_{\text{region}}$ is a mention-masked BCE between region logits and pooled region targets.

\paragraph{Concept and region quality.}
We evaluate concept prediction quality using micro-averaged classification metrics and AUROC/AUPRC computed over the subset of concepts with non-masked targets.
When region prediction is enabled, we analogously compute region metrics using pooled region targets derived from report supervision and report per-region concept quality by grouping concepts under $g(k)$.

\paragraph{Stage 2: concepts to labels.}
After training the concept predictor, we export per-study concept probabilities and train a diagnosis head on these predicted concepts.
We consider a flat CBM head (a small MLP) and a hierarchical region-gated head described next.
The diagnosis head is trained with the concept predictor frozen, so performance reflects the quality of the learned concept bottleneck rather than end-to-end fine-tuning.

\paragraph{Hierarchical region gating with linear label head.}
The hierarchical head predicts region logits $u=W_r\hat{c}$ and region probabilities
\begin{equation}
\label{eq:region_probs}
\hat{z}=\sigma\!\left(\frac{u}{\tau}\right),\qquad
\hat{z}\leftarrow \epsilon+(1-\epsilon)\hat{z},
\end{equation}
where $\tau$ is a temperature and $\epsilon$ is an optional gate floor for conservative soft-gating.
Each concept is gated by its parent region, yielding $\hat{c}^{\,\text{gated}}_k=\hat{z}_{g(k)}\,\hat{c}_k$.
Labels are then predicted by a bias-free linear layer
\begin{equation}
\label{eq:linear_label_head}
\ell=W_y\hat{c}^{\,\text{gated}}.
\end{equation}
This constraint makes explanations intrinsic: the signed contribution of concept $k$ to label $\ell_j$ is $W_y[j,k]\;\hat{c}^{\,\text{gated}}_k$.
Removing the bias term prevents the head from predicting a diagnosis in the absence of supporting concept evidence.

\paragraph{Gate clamping and region consistency.}
During diagnosis-head training, we derive soft region targets by pooling the input concept vector, $P_r(\hat{c})$, and optionally clamp region gates by multiplying $\hat{z}$ with these pooled targets.
We also optionally include an auxiliary region loss that encourages region logits to match pooled targets.
The full objective is
\begin{equation}
\label{eq:cbm_loss}
\mathcal{L}_{\text{CBM}}=\mathcal{L}_{\text{label}}+\lambda_r\mathcal{L}_{\text{region}},
\end{equation}
where $\mathcal{L}_{\text{label}}$ is a masked BCE on CheXpert-style labels and $\mathcal{L}_{\text{region}}$ is a BCE between $u$ and pooled region targets.
We train both stages with Adam, use early stopping based on validation performance, and export per-study concept probabilities (and region probabilities when enabled) for downstream training and inspection.

\subsection{Interpretability and Concept Interventions}
\radcbm\ produces structured intermediate outputs that support direct inspection.
First, we report the predicted concept vector $\hat{c}$ as an ontology-grounded explanation.
Second, the region head produces region probabilities $\hat{z}$ that summarize abnormality by coarse anatomy and control which findings can contribute to predictions.
Third, for the hierarchical head, Eq.~(\ref{eq:linear_label_head}) yields intrinsic attributions via the per-concept contributions $W_y[j,k]\hat{c}^{\,\text{gated}}_k$.

\paragraph{Anatomical plausibility.}
We quantify anatomical plausibility by measuring the rate of \emph{implausible activations}, defined as cases where a concept is predicted present while its parent region gate is low.
Operationally, for a threshold $\delta$, an activation is implausible if $\hat{c}_k>\delta$ but $\hat{z}_{g(k)}<\delta$.

\paragraph{Concept interventions.}
To assess intervention behavior, we edit selected concept entries in $\hat{c}$ (setting $\hat{c}_k$ to 0 or 1) and measure the induced change in predicted label probabilities.
In intrinsic CBMs, the signed contributions in Eq.~(\ref{eq:linear_label_head}) provide a testable prediction for the direction and relative magnitude of these counterfactual effects.

\paragraph{Gate plausibility and intervention summaries.}
In addition to standard label metrics, we summarize gate plausibility by aggregating implausible activation rates across concepts and across regions.
For trained hierarchical heads, we also report compact intervention summaries by ranking concepts by the magnitude of their learned contributions and estimating the expected change in label logits under concept edits.
