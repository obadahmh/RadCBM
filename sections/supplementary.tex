% =============================================================================
% RadCBM Supplementary Material (Revised)
% =============================================================================

\providecommand{\placeholder}[1]{\textbf{#1}}
\providecommand{\radcbm}{RadCBM}
\providecommand{\maybeincludegraphics}[2]{%
  \IfFileExists{#1}{%
    \includegraphics[#2]{#1}%
  }{%
    \fbox{\parbox[c][0.22\textheight][c]{0.95\linewidth}{\centering Missing figure: \texttt{\detokenize{#1}}}}%
  }%
}


\subsection{Evaluation Label Provenance}

Table~\ref{tab:label_provenance} summarizes which benchmarks provide radiologist-annotated evaluation labels versus report-derived labels.

\begin{table}[H]
\centering
\caption{Evaluation label source at test time.}
\label{tab:label_provenance}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lccc}
\toprule
\textbf{Benchmark} & \textbf{Label Source} & \textbf{Level} & \textbf{Label Set} \\
\midrule
MIMIC-CXR & radiologist-annotated (reports; test split) & study & CheXpert-14 \\
CheXpert Plus & radiologist-annotated & study & CheXpert-14 \\
VinDr-CXR & radiologist-annotated & image+bbox & VinDr-28 (mapped to CheXpert-5) \\
RSNA Pneumonia & radiologist-annotated & image+bbox & Pneumonia \\
NIH ChestX-ray14 & report-derived & image & NIH-14 \\
\bottomrule
\end{tabular}
}
\end{table}

For MIMIC-CXR, we evaluate using radiologist-annotated report labels released with the PhysioNet MIMIC-CXR-JPG test split (CheXpert-14 categories). These are report annotations rather than independent radiologist re-reads of the images. NIH ChestX-ray14 labels are report-derived; we treat them as complementary evidence and emphasize radiologist-labeled subsets as primary validation.

\subsection{Region Consistency Metric}

We quantify alignment between coarse region gates and fine concept evidence using:
\begin{equation}
\label{eq:region_consistency}
\mathrm{RC} = 1 - \frac{1}{N|\mathcal{R}|} \sum_{i=1}^{N} \sum_{r \in \mathcal{R}} \left| (\hat{z}_i)_r - \max_{k: g(k)=r} (\hat{c}_i)_k \right|.
\end{equation}
Values near 1 indicate that region gates faithfully summarize the underlying concept activations.

\subsection{Ablation Study}

Table~\ref{tab:ablation} presents an incremental ablation isolating key design choices. We add components to a base \radcbm\ model while keeping the evaluation protocol fixed (thresholds tuned on MIMIC-CXR validation, then frozen).

\begin{table}[H]
\centering
\caption{Ablation study on MIMIC-CXR test set. Results averaged over 3 seeds.}
\label{tab:ablation}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcccc}
\toprule
\textbf{Configuration} & \textbf{Macro} & \textbf{Concept} & \textbf{Plaus.} & \textbf{Interv.} \\
 & \textbf{AUC} & \textbf{AUC} & & \textbf{Faith.} \\
\midrule
\radcbm\ (base) & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XX} \\
+ Mention masking & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XX} \\
+ Assertion-aware targets & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XX} \\
+ Hierarchical gating & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XX} \\
+ Conservative soft-gating ($\epsilon > 0$) & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XX} \\
\midrule
\radcbm\ (full) & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XX} \\
\bottomrule
\end{tabular}
}
\end{table}

\textbf{Mention masking} excludes unmentioned concepts from supervision rather than treating them as negatives, improving concept AUC by reducing label noise. \textbf{Assertion-aware targets} maps present/absent/uncertain assertions to 1/0/0.5 rather than binary labels, providing softer supervision for ambiguous findings. \textbf{Hierarchical gating} introduces region-level gates that suppress anatomically implausible concept activations. \textbf{Conservative soft-gating} sets $\epsilon > 0$ to prevent gates from fully closing, avoiding cascading failures where a missed region prediction suppresses all constituent findings.

\subsection{Region-Level Performance}

Table~\ref{tab:region_performance} reports performance decomposed by anatomical region. Region AUC measures binary abnormality detection using surrogate targets obtained by max-pooling concept assertions per region. Finding AUC measures concept prediction within each region.

\begin{table}[H]
\centering
\caption{Region-level performance on MIMIC-CXR test set. Results averaged over 3 seeds. The ``other'' category (581 concepts) is excluded as it aggregates heterogeneous findings without clear anatomical localization.}
\label{tab:region_performance}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcccc}
\toprule
\textbf{Region} & \textbf{\#Concepts} & \textbf{Region AUC} & \textbf{Finding AUC} & \textbf{Prevalence (\%)} \\
\midrule
Lung & 259 & 0.684$\pm$0.000 & 0.691$\pm$0.000 & 92.1 \\
Heart & 143 & 0.822$\pm$0.000 & 0.770$\pm$0.000 & 81.1 \\
Pleura & 69 & 0.727$\pm$0.000 & 0.698$\pm$0.000 & 81.9 \\
Mediastinum & 116 & 0.592$\pm$0.000 & 0.701$\pm$0.000 & 89.7 \\
Bone & 144 & 0.702$\pm$0.000 & 0.676$\pm$0.000 & 78.6 \\
\midrule
\textbf{Overall} & 1,312 & 0.678$\pm$0.000 & 0.693$\pm$0.000 & --- \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Learned Concept-Label Relationships}

\begin{figure}[H]
\centering
\maybeincludegraphics{figures/concept_label_weights.pdf}{width=\linewidth}
\caption{Learned concept-to-label weights from the linear diagnosis head. Each row shows the top-5 positive and top-5 negative concept contributions for one CheXpert label.}
\label{fig:concept_weights}
\end{figure}

\subsection{Concept Bank Statistics}

\begin{figure}[H]
\centering
\maybeincludegraphics{figures/concept_bank_stats.pdf}{width=\linewidth}
\caption{Concept bank statistics. (a) Concept frequency distribution (log scale); vertical lines indicate CheXpert-14 concept positions. (b) Hierarchical organization by anatomical region. (c) Vocabulary coverage comparison with prior work.}
\label{fig:concept_bank}
\end{figure}

\subsection{Effect of Hierarchical Gating}

\begin{figure}[H]
\centering
\maybeincludegraphics{figures/gating_effect.pdf}{width=\linewidth}
\caption{Effect of hierarchical gating. (a) Region abnormality score versus mean finding activation for flat vs hierarchical variants. (b) Distribution of finding activations stratified by region gate status.}
\label{fig:gating_effect}
\end{figure}

\subsection{Intervention Faithfulness Analysis}

\begin{figure}[H]
\centering
\maybeincludegraphics{figures/intervention_faithfulness.pdf}{width=\linewidth}
\caption{Intervention faithfulness. (a) Label probability as a function of concept activation. (b) Predicted versus observed label change upon concept intervention.}
\label{fig:intervention}
\end{figure}

\subsection{Hyperparameter Sensitivity}

\begin{figure}[H]
\centering
\maybeincludegraphics{figures/hyperparameter_sensitivity.pdf}{width=0.9\linewidth}
\caption{Sensitivity to region loss weight $\lambda_r$. Validation macro AUC remains stable across $\lambda_r \in [0.01, 1.0]$.}
\label{fig:hyperparameter}
\end{figure}

\subsection{Concept AUC by Frequency}

\begin{figure}[H]
\centering
\maybeincludegraphics{figures/concept_auc_by_frequency.pdf}{width=0.9\linewidth}
\caption{Concept AUC stratified by training set frequency. Hierarchical gating provides larger gains for rare concepts.}
\label{fig:concept_frequency}
\end{figure}

\subsection{Qualitative Case Studies}

\begin{figure*}[H]
\centering
\maybeincludegraphics{figures/qualitative_cases.pdf}{width=\textwidth}
\caption{Qualitative examples illustrating region-aware explanations. For each case, we show the input radiograph, predicted region gates, top activated concepts, and diagnosis with concept contributions.}
\label{fig:qualitative}
\end{figure*}

\subsection{Cross-Dataset Generalization}

\begin{table}[H]
\centering
\caption{Cross-dataset generalization. Models trained on one dataset and evaluated on another. $\Delta$ indicates change from in-domain performance.}
\label{tab:generalization}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llcc}
\toprule
\textbf{Method} & \textbf{Train $\rightarrow$ Test} & \textbf{Macro AUC} & \textbf{$\Delta$} \\
\midrule
DenseNet-121 & MIMIC $\rightarrow$ CheXpert & 0.521 & $-23.8\%$ \\
\radcbm\ & MIMIC $\rightarrow$ CheXpert & 0.526 & $-33.7\%$ \\
\midrule
DenseNet-121 & CheXpert $\rightarrow$ MIMIC & 0.684 & $+31.3\%$ \\
\radcbm\ & CheXpert $\rightarrow$ MIMIC & 0.659 & $+18.9\%$ \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Computational Requirements}

\begin{table}[H]
\centering
\caption{Computational requirements on MIMIC-CXR. Params count trainable parameters only (frozen MedCLIP backbones are shared across CBMs and excluded). Inference measured on NVIDIA RTX 3080 with batch size 1 using the full pipeline.}
\label{tab:computational}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Params (M)} & \textbf{Inference (ms)} & \textbf{Training (GPU-hrs)} \\
\midrule
DenseNet-121 & 7.0 & 9.9 & 0.0 \\
AdaCBM & 0.5 & 9.5 & 0.02 \\
\radcbm\ (flat) & 4.4 & 12.7 & 2.1 \\
\radcbm\ & 2.7 & 16.6 & 2.3 \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Error Analysis}

\begin{figure}[H]
\centering
\maybeincludegraphics{figures/error_analysis.pdf}{width=\linewidth}
\caption{Error analysis. (a) Region-level confusion matrix. (b) False-negative cascade: missed findings due to incorrect region normality prediction.}
\label{fig:error_analysis}
\end{figure}

\subsection{Calibration}

\begin{table}[H]
\centering
\caption{Calibration on CheXpert Plus expert subset. Lower is better.}
\label{tab:calibration}
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{ECE} $\downarrow$ & \textbf{Brier} $\downarrow$ \\
\midrule
DenseNet-121 & 0.287 & 0.291 \\
MedCLIP & 0.174 & 0.237 \\
\radcbm\ & 0.430 & 0.402 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\maybeincludegraphics{figures/reliability_diagram.pdf}{width=\linewidth}
\caption{Reliability diagram on CheXpert Plus expert subset.}
\label{fig:reliability}
\end{figure}

\subsection{Rare-Label Performance}

\begin{table}[H]
\centering
\caption{PR-AUC on rare labels (CheXpert Plus expert subset). Lung Lesion and Pleural Other have no positives in this subset, so values are reported as --- and excluded from the macro average.}
\label{tab:prauc}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcccccc}
\toprule
\textbf{Method} & \rotatebox{90}{Fracture} & \rotatebox{90}{Pneumothorax} & \rotatebox{90}{Pneumonia} & \rotatebox{90}{Lung Lesion} & \rotatebox{90}{Pleural Other} & \textbf{Macro} \\
\midrule
DenseNet-121 & 0.69 & 0.26 & 0.92 & --- & --- & 0.623 \\
MedCLIP & 0.54 & 0.27 & 1.00 & --- & --- & 0.602 \\
\radcbm\ & 0.79 & 0.25 & 0.92 & --- & --- & 0.650 \\
\bottomrule
\end{tabular}%
}
\end{table}

\subsection{Uncertainty Handling Protocol}

Unless otherwise stated, we map labeler outputs to \{positive, negative, uncertain\}. For training labels derived from reports, we optionally use an ensemble of labelers (CheXpert, CheXbert, NegBio); disagreements are marked uncertain. For disease labels, uncertain values are treated as missing and excluded from loss and evaluation. For concept targets, uncertain assertions receive soft targets (0.5). Decision thresholds are tuned on MIMIC-CXR validation and fixed for all evaluations.

\begin{table}[H]
\centering
\caption{Sensitivity to disease-label uncertainty handling on CheXpert Plus expert subset (macro AUC).}
\label{tab:uncertainty_sensitivity}
\begin{tabular}{lc}
\toprule
\textbf{Setting} & \textbf{Macro AUC} \\
\midrule
U-Ignore (default) & 0.526 \\
U-Zero & 0.526 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Full CheXpert-14 Results}

Table~\ref{tab:classification_chexpert14} reports per-label AUC on all 14 CheXpert observations, complementing the main paper's focus on the five competition labels.

\begin{table*}[H]
\centering
\caption{Full CheXpert-14 classification (AUC-ROC) on MIMIC-CXR test and CheXpert Plus validation. Best in \textbf{bold}, second-best \underline{underlined}. All CBMs share the same frozen backbone. Mean over 3 seeds; std $<$0.01 omitted. DenseNet-121 does not predict No Finding, Pleural Other, or Support Devices, so those entries are reported as --- and excluded from the macro. Labels without positives in the CheXpert Plus expert subset are reported as --- and excluded from the macro.}
\label{tab:classification_chexpert14}
\resizebox{\textwidth}{!}{%
\begin{tabular}{llcccccccccccccc|c}
\toprule
\textbf{Method} & \textbf{Type} & \rotatebox{90}{Atelect.} & \rotatebox{90}{Cardiom.} & \rotatebox{90}{Consolid.} & \rotatebox{90}{Edema} & \rotatebox{90}{Enl. Card.} & \rotatebox{90}{Fracture} & \rotatebox{90}{Lung Les.} & \rotatebox{90}{Lung Op.} & \rotatebox{90}{No Finding} & \rotatebox{90}{Pl. Eff.} & \rotatebox{90}{Pl. Other} & \rotatebox{90}{Pneumonia} & \rotatebox{90}{Pneumothor.} & \rotatebox{90}{Supp. Dev.} & \textbf{Macro} \\
\midrule
\multicolumn{17}{l}{\textit{MIMIC-CXR Test Set}} \\
\midrule
DenseNet-121 & CNN & 0.62 & 0.80 & 0.74 & 0.80 & 0.74 & 0.50 & 0.52 & 0.61 & --- & 0.80 & --- & 0.65 & 0.58 & --- & 0.669 \\
MedCLIP & VLM & 0.58 & \underline{0.87} & 0.88 & 0.84 & 0.53 & 0.61 & 0.45 & \textbf{0.73} & --- & \textbf{0.92} & --- & 0.57 & 0.75 & 0.80 & 0.712 \\
\midrule
LaBo & CBM & 0.59 & 0.76 & 0.87 & 0.70 & 0.66 & 0.44 & \textbf{0.75} & 0.71 & --- & 0.71 & --- & \underline{0.76} & 0.43 & 0.80 & 0.682 \\
AdaCBM & CBM & \underline{0.73} & \textbf{0.88} & \textbf{0.88} & 0.84 & 0.72 & 0.42 & 0.64 & 0.65 & --- & \underline{0.91} & --- & \textbf{0.77} & 0.76 & 0.76 & 0.747 \\
C2F-CBM & H-CBM & \textbf{0.77} & 0.74 & 0.76 & 0.86 & 0.64 & \textbf{0.62} & \underline{0.71} & 0.71 & \textbf{0.81} & 0.88 & \textbf{0.83} & 0.72 & \textbf{0.86} & \textbf{0.89} & \underline{0.772} \\
\midrule
\radcbm\ (flat) & CBM & 0.72 & 0.85 & 0.88 & \underline{0.87} & \underline{0.74} & 0.57 & 0.67 & 0.70 & --- & 0.91 & --- & 0.75 & 0.76 & 0.81 & 0.769 \\
\radcbm\ & H-CBM & 0.71 & 0.85 & \underline{0.88} & \textbf{0.88} & \textbf{0.74} & \underline{0.61} & 0.68 & \underline{0.72} & --- & 0.91 & --- & 0.75 & \underline{0.77} & \underline{0.82} & \textbf{0.777} \\
\midrule
\multicolumn{17}{l}{\textit{CheXpert Plus Expert Subset}} \\
\midrule
DenseNet-121 & CNN & 0.53 & 0.50 & 0.53 & \underline{0.55} & 0.52 & 0.40 & --- & \textbf{0.50} & --- & \textbf{0.58} & --- & \underline{0.67} & 0.55 & --- & 0.532 \\
MedCLIP & VLM & \underline{0.60} & 0.40 & 0.40 & 0.48 & \underline{0.62} & 0.20 & --- & \underline{0.50} & --- & 0.52 & --- & \textbf{1.00} & 0.51 & \textbf{0.77} & 0.546 \\
\midrule
LaBo & CBM & 0.51 & 0.53 & 0.41 & 0.48 & \textbf{0.65} & \textbf{0.73} & --- & 0.46 & --- & 0.51 & --- & \underline{0.67} & 0.48 & \underline{0.74} & 0.561 \\
AdaCBM & CBM & 0.46 & 0.51 & \textbf{0.57} & 0.52 & 0.57 & \underline{0.69} & --- & 0.45 & --- & 0.55 & --- & \underline{0.67} & \underline{0.55} & 0.66 & \textbf{0.563} \\
C2F-CBM & H-CBM & \textbf{0.60} & 0.42 & 0.46 & \textbf{0.59} & 0.51 & 0.53 & \textbf{0.40} & 0.47 & \textbf{0.52} & 0.53 & \textbf{0.41} & 0.54 & \textbf{0.59} & 0.49 & 0.505 \\
\midrule
\radcbm\ (flat) & CBM & 0.51 & \underline{0.53} & 0.53 & 0.50 & 0.59 & \underline{0.69} & --- & 0.42 & --- & 0.57 & --- & \underline{0.67} & 0.52 & 0.67 & \underline{0.563} \\
\radcbm\ & H-CBM & 0.52 & \textbf{0.54} & \underline{0.53} & 0.50 & 0.59 & 0.67 & --- & 0.40 & --- & \underline{0.57} & --- & \underline{0.67} & 0.53 & 0.68 & 0.563 \\
\bottomrule
\end{tabular}%
}
\end{table*}
