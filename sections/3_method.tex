\subsection{Pipeline Overview}
Figure~\ref{fig:pipeline} summarizes the training pipeline.
During training, paired chest X-rays and reports are processed jointly: reports are converted into soft concept targets, and images provide pixel-level evidence for those concepts and the final diagnostic labels.
Images pass through a convolutional backbone to produce features that feed a two-level CBM with region abnormality heads, region-specific finding heads, multiplicative gating, and a linear label predictor.
Reports are only used to build and supervise the concept layer; at test time, \radcbm\ receives only images and outputs region scores, concept activations, and label predictions.
\begin{figure*}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{method/system_overview_v1_compressed.png}
    \caption{RadCBM training pipeline.
    Reports are converted into normalized concept targets and grouped into an anatomy-first hierarchy.
    Images are mapped to region abnormality and finding predictions; multiplicative gating yields gated concepts that feed a linear label head.}
    \label{fig:pipeline}
\end{figure*}

\subsection{Concept Extraction from Reports}
Each report is parsed with RadGraph to identify observation and anatomy entities plus their relations (for example, \texttt{located\_at}).
For each observation-anatomy pair, we treat the combination as a candidate visual concept anchored to a specific region in the image.
Entities are normalized to canonical SNOMED CT concepts in the UMLS, and only clinically meaningful semantic types are kept: imaging observations, clinical findings, pathophysiologic processes, and chest anatomy.
We discard modifiers unrelated to visual evidence (for example, modality, procedure, and report-structure tokens) and drop entities that cannot be mapped confidently to chest anatomy.
Frequent concepts (occurring at least 50 times in training reports) form the vocabulary; highly correlated terms and obvious synonyms are merged to avoid redundancy while preserving distinct clinical meanings.

For a given report, we construct (1)~a soft concept target vector $t \in \{0, 0.5, 1\}^N$ and (2)~a supervision mask $m \in \{0,1\}^N$ that distinguishes explicit negation from missing mentions:
\[
(t_i, m_i) =
\begin{cases}
(1, 1), & \text{if concept $i$ is definitely present},\\
(0.5, 1), & \text{if concept $i$ is uncertain},\\
(0, 1), & \text{if concept $i$ is explicitly absent (negated)},\\
(0, 0), & \text{if concept $i$ is unmentioned in the report}.
\end{cases}
\]
Here, ``definitely present'' corresponds to positive, non-negated mentions in the report, while ``uncertain'' captures hedged or equivocal language (for example, ``may represent'' or ``cannot exclude'').
Crucially, we do not treat unmentioned concepts as negatives: $m_i = 0$ excludes them from the finding loss, whereas explicitly negated mentions provide reliable negative supervision ($t_i = 0, m_i = 1$).
Region-level targets $A_r$ are set to one if any finding linked to region $r$ is present or uncertain.
Region supervision is therefore derived automatically: for each study, we mark a region as abnormal if any present or uncertain concept is linked to that region in the RadGraph/UMLS extraction.
We do not curate separate region abnormality labels.

For downstream classification (for example, CheXpert-14 and the 5 CheXpert competition labels), label supervision is likewise derived from reports rather than manual image annotation and should be treated as pseudo-labels. We use the CheXpert labeler to produce per-study labels and optionally ensemble multiple report labelers (CheXpert, CheXbert, and NegBio), treating disagreements as uncertain and handling them as either soft targets or masked labels during training.

\subsection{Hierarchical Concept Vocabulary}
Concepts are organized into two levels that mirror radiologist reasoning.
Level~1 contains coarse anatomical regions: lungs, heart, pleura, mediastinum, and bone, plus optional catch-all regions for devices and other structures when needed.
Level~2 contains findings specific to each region (for example, opacity, consolidation, nodule, and atelectasis in the lungs; effusion and pneumothorax in the pleura; cardiomegaly in the heart).
Some concepts that are meaningful in multiple regions (for example, ``mass'') are duplicated with region-specific identifiers to keep explanations localized.
Grouping findings under regions allows the model to express explanations as ``region abnormal'' followed by the most likely findings within that region, which matches how radiologists structure reports and facilitates downstream presentation of explanations.

\subsection{Model Architecture}
Let $x$ denote an image and $h = \phi(x)$ the backbone features.
We instantiate $\phi$ as a radiology-pretrained vision encoder (for example, a CLIP-style ViT trained on chest X-ray image--report pairs), and use its pooled image representation as $h$ so that it summarizes the image in a fixed-length vector.
Region abnormality scores are predicted as
\begin{equation}
a = \sigma(W_a h + b_a), \quad a \in [0,1]^K,
\end{equation}
where $K$ is the number of regions.
For each region $r$, finding probabilities are
\begin{equation}
f_r = \sigma(W_r h + b_r), \quad f_r \in [0,1]^{N_r},
\end{equation}
and multiplicative gating enforces clinical consistency:
\begin{equation}
c_r = a_r \odot f_r.
\end{equation}
The full concept vector is $c = [c_1; \ldots; c_K] \in [0,1]^N$.
A linear label head maps concepts to diagnostic logits,
\begin{equation}
\hat{y} = W_y c + b_y,
\end{equation}
so each weight directly reflects how a concept contributes to a class.
Because $c_r$ is explicitly gated by $a_r$, a high finding probability $f_r$ cannot influence the label prediction unless the corresponding region is predicted abnormal, enforcing a simple, clinically motivated dependency structure between regions, findings, and labels.

\subsection{Training Objectives}
Training optimizes a weighted sum of label, region, and finding losses:
\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{cls}} + \lambda_1 \mathcal{L}_{\text{region}} + \lambda_2 \mathcal{L}_{\text{finding}}.
\end{equation}
Label prediction uses cross-entropy on $\hat{y}$.
Region supervision uses binary cross-entropy between $a_r$ and $A_r$.
Finding supervision uses masked binary cross-entropy between $f_r$ and $t_i$ for concepts in region $r$, applying loss only when $m_i=1$ so unmentioned concepts are not treated as negatives; uncertain targets ($t_i = 0.5$) are down-weighted.
Positive class weights address the natural class imbalance where most findings are absent.
Losses are applied to $f_r$ (not $c_r$) so gradients flow even when gating is low.

Backbone weights are fine-tuned from the chosen radiology-pretrained checkpoint.
Region and finding heads are randomly initialized.
Temperatures or thresholds per concept can be calibrated on validation data when needed.
We train all models with the same optimization hyperparameters (Adam optimizer, fixed learning rate, and mini-batch training) so that differences in performance can be attributed to the choice of concept vocabulary and architecture rather than to training instability.

\subsection{Inference and Explanations}
At test time, only images are required.
The model outputs region abnormality scores, gated finding probabilities, and label logits.
Explanations are derived from the linear label head: the contribution of concept $i$ to class $\ell$ is $c_i \cdot W_y[\ell, i]$.
Summing contributions within a region provides region-level rationales, and sorting concepts by contribution magnitude yields concise lists of the most supportive and most contradicting findings for each label.
Manual concept edits correspond to overriding $c_i$ before the linear head (for example, forcing effusion to zero or setting cardiomegaly to one) and recomputing $\hat{y}$, enabling counterfactual testing that directly reflects how manipulating specific findings would change the prediction.
