\subsection{Pipeline Overview}
Figure~\ref{fig:pipeline} summarizes the training pipeline.
Paired chest X-rays and reports are processed to extract concept targets.
Images pass through a convolutional backbone to produce features that feed a two-level CBM: region abnormality heads, region-specific finding heads, multiplicative gating, and a linear label predictor.
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{method/system_overview_compressed.pdf}
    \caption{RadCBM training pipeline.
    Reports are converted into normalized concept targets and grouped into an anatomy-first hierarchy.
    Images are mapped to region abnormality and finding predictions; multiplicative gating yields gated concepts that feed a linear label head.}
    \label{fig:pipeline}
\end{figure}

\subsection{Concept Extraction from Reports}
Each report is parsed with RadGraph to identify observation and anatomy entities plus their relations (for example, \texttt{located\_at}).
Entities are normalized to canonical RadLex or UMLS terms, and only clinically meaningful semantic types are kept: imaging observations, clinical findings, pathophysiologic processes, and chest anatomy.
We discard modifiers unrelated to visual evidence (for example, modality or procedure).
Frequent concepts (occurring at least 50 times in training reports) form the vocabulary; highly correlated terms are merged to avoid redundancy.

For a given report, we construct a soft concept target vector $t \in \{0, 0.5, 1\}^N$:
\[
t_i =
\begin{cases}
1, & \text{if concept $i$ is definitely present},\\
0.5, & \text{if concept $i$ is uncertain},\\
0, & \text{if concept $i$ is absent or unmentioned}.
\end{cases}
\]
Region-level targets $A_r$ are set to one if any finding linked to region $r$ is present or uncertain.

\subsection{Hierarchical Concept Vocabulary}
Concepts are organized into two levels that mirror radiologist reasoning.
Level~1 contains anatomical regions (lungs, heart, pleura, mediastinum, bone).
Level~2 contains findings specific to each region (for example, opacity, consolidation, effusion, pneumothorax).
Grouping findings under regions allows the model to express explanations as ``region abnormal'' followed by the most likely findings within that region.

\subsection{Model Architecture}
Let $x$ denote an image and $h = \phi(x)$ the backbone features.
Region abnormality scores are predicted as
\begin{equation}
a = \sigma(W_a h + b_a), \quad a \in [0,1]^K,
\end{equation}
where $K$ is the number of regions.
For each region $r$, finding probabilities are
\begin{equation}
f_r = \sigma(W_r h + b_r), \quad f_r \in [0,1]^{N_r},
\end{equation}
and multiplicative gating enforces clinical consistency:
\begin{equation}
c_r = a_r \odot f_r.
\end{equation}
The full concept vector is $c = [c_1; \ldots; c_K] \in [0,1]^N$.
A linear label head maps concepts to diagnostic logits,
\begin{equation}
\hat{y} = W_y c + b_y,
\end{equation}
so each weight directly reflects how a concept contributes to a class.

\subsection{Training Objectives}
Training optimizes a weighted sum of label, region, and finding losses:
\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{cls}} + \lambda_1 \mathcal{L}_{\text{region}} + \lambda_2 \mathcal{L}_{\text{finding}}.
\end{equation}
Label prediction uses cross-entropy on $\hat{y}$.
Region supervision uses binary cross-entropy between $a_r$ and $A_r$.
Finding supervision uses binary cross-entropy between $f_r$ and $t_i$ for concepts in region $r$; uncertain targets ($t_i = 0.5$) are down-weighted.
Positive class weights address the natural class imbalance where most findings are absent.
Losses are applied to $f_r$ (not $c_r$) so gradients flow even when gating is low.

Backbone weights are fine-tuned from ImageNet-pretrained DenseNet-121.
Region and finding heads are randomly initialized.
Temperatures or thresholds per concept can be calibrated on validation data when needed.

\subsection{Inference and Explanations}
At test time, only images are required.
The model outputs region abnormality scores, gated finding probabilities, and label logits.
Explanations are derived from the linear label head: the contribution of concept $i$ to class $\ell$ is $c_i \cdot W_y[\ell, i]$.
Summing contributions within a region provides region-level rationales, and manual concept edits enable counterfactual testing (for example, forcing effusion to zero to see its effect on the prediction).
