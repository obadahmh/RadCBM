\begin{abstract}
Chest X-ray classifiers remain hard to trust because their reasoning is hidden and the dense concept supervision they would need for transparent explanations is prohibitively expensive to obtain at scale.
\radcbm\ is a hierarchical concept bottleneck model that replaces manual per-image concept labels with concept targets mined directly from paired radiology reports.
We extract concepts with RadGraph, normalize them to RadLex/UMLS terms, filter to clinically meaningful semantic types, and organize them into anatomy-level regions.
This yields a clinically grounded vocabulary of visual, image-evident concepts spanning lung, heart, pleura, mediastinum, and bone.
A two-level predictor first estimates region abnormality, then predicts region-specific findings gated by those regions; a linear label head maps the gated concepts to diagnostic labels, so that each prediction decomposes into contributions from named regions and findings and supports direct counterfactual editing.
On MIMIC-CXR and CheXpert, automated annotations cover the long tail of radiographic findings without human curation, and the hierarchical CBM improves concept AUC and reduces implausible activations compared to flat CBMs and CheXpert-style concept sets.
Classification performance matches a black-box DenseNet-121 while exposing per-region rationales whose intervention effects are faithful to the learned decision boundary.
\radcbm\ turns routine reports into training signals, aligning model decisions with radiologist workflows and providing region-aware, concept-level explanations without sacrificing accuracy.
\end{abstract}
