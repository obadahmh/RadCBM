
\begin{abstract}
Understanding the inner workings of deep neural networks (DNNs) has become increasingly critical, particularly for applications demanding transparency and accountability. Concept-based interpretable models have emerged as a promising solution, wherein images are first mapped into a human-understandable concept space and then linearly combined to yield predictions. This ensures the alignment of explanations with the prediction process. However, the widespread adoption of these methods is impeded by their reliance on datasets with predefined concept annotations, a requirement that is both time-consuming and impractical.

Tending to this demand, this article...

% We propose herein

\end{abstract}
