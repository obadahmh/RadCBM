\subsection{Deep Learning for Chest Radiography}

Progress in chest radiograph interpretation has been driven by large-scale datasets including ChestX-ray14~\cite{wang2017chestxray14}, CheXpert~\cite{irvin2019chexpert}, and MIMIC-CXR~\cite{johnson2019mimiccxr}. Classification architectures have evolved from DenseNet-based models~\cite{rajpurkar2017chexnet} to Vision Transformers~\cite{shamshad2023transformers}, with recent foundation models such as Ark+ demonstrating strong generalization to rare and novel diseases~\cite{ma2025arkplus}. Parallel work has pursued vision-language pretraining: ConVIRT~\cite{zhang2022convirt}, MedCLIP~\cite{wang2022medclip}, and BiomedCLIP~\cite{zhang2023biomedclip} align radiograph and report embeddings for zero-shot transfer. These approaches achieve high accuracy but produce entangled representations that lack explicit clinical semantics, motivating concept-based alternatives.

\subsection{Concept Bottleneck Models}

Koh et al.~\cite{koh2020concept} introduced Concept Bottleneck Models (CBMs), which route predictions through interpretable intermediate concepts. Subsequent work has relaxed the strict bottleneck via concept embeddings~\cite{zarlenga2022cem}, enabled post-hoc retrofitting of pretrained networks~\cite{yuksekgonul2023posthoc}, and supported test-time concept intervention~\cite{chauhan2023interactive}. Coarse-to-fine architectures tie global predictions to localized findings~\cite{panousis2024coarsetofineconceptbottleneckmodels}. The persistent bottleneck is supervision: label-free CBMs~\cite{oikarinen2023labelfree,yang2023language} align CLIP representations with concept predictors to avoid manual annotation, while visual concept filtering~\cite{kim2023visualconceptfiltering} prunes LLM-generated vocabularies to visually grounded subsets. However, Debole et al.~\cite{debole2025vlmcbm} show that VLM-derived supervision diverges substantially from expert annotations and that concept accuracy does not correlate with downstream task performance, underscoring the need for higher-quality concept sources. In radiology, AHIVE~\cite{yan2024ahive} organizes visual features by anatomical region for report retrieval. Our work addresses concept quality directly by constructing the concept bank from structured report extraction and ontology grounding rather than VLM weak supervision.

\subsection{Concept Extraction from Radiology Reports}

Rule-based extractors such as NegBio~\cite{peng2018negbio} and the CheXpert labeler~\cite{irvin2019chexpert} identify findings and negation via pattern matching; CheXbert~\cite{smit2020chexbert} improved accuracy with BERT fine-tuning. RadGraph~\cite{jain2021radgraph} extended extraction to entity-relation graphs over observations and anatomy. RadGraph-XL~\cite{delbrouck2024radgraphxl} scaled annotation to 2,300 reports across four modalities; RadGraph2~\cite{khanna2023radgraph2} added temporal change tracking. For entity linking, SapBERT~\cite{liu2021sapbert} achieves strong performance grounding mentions to UMLS~\cite{bodenreider2004umls}. These tools are widely used for evaluating generated reports via RadGraph F1, but their structured outputs have not been repurposed as CBM supervision. We bridge this gap.