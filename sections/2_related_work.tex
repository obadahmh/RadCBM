Post-hoc interpretability methods, such as saliency maps and feature attributions, highlight image regions that influence a model's output.
They preserve black-box accuracy but often fail faithfulness checks and offer limited insight into the model's internal logic.
Counterfactual explanations improve causal grounding but still operate after training.

Concept bottleneck models make interpretability part of the architecture: images are mapped to human-understandable concepts that are then used to predict labels.
Early CBMs relied on manually annotated concepts and often traded accuracy for transparency.
Recent variants predict concepts post-hoc or with weak supervision, but they either depend on small vocabularies (for example, the CheXpert labeler) or on vision-language models that are not calibrated for clinical use.
Flat CBMs also ignore the anatomy-first reasoning radiologists employ, leading to implausible concept activations.

Radiology report mining offers a scalable alternative.
Tools like RadGraph extract observation and anatomy entities plus relations from free text.
Prior work has used report-derived labels to supervise black-box classifiers, but the labels are coarse and do not expose the reasoning process.
RadCBM combines report-derived concepts with a hierarchical, gated CBM so that explanations remain both faithful to the model and aligned with clinical reading workflows.
