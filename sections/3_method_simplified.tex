% =============================================================================
% RadCBM Methods Section (Restructured)
% =============================================================================

\radcbm\ learns to classify chest radiographs through an interpretable bottleneck of clinically meaningful concepts. Rather than predicting diagnoses directly from image features, our model first predicts the presence of specific radiological findings (pulmonary edema, cardiomegaly, pleural effusion, and so on), then uses only these predicted concepts to make diagnostic decisions. This two-stage design makes the model's reasoning transparent: for any prediction, we can inspect exactly which concepts the model detected and how much each contributed to the final diagnosis.

The key challenge is obtaining concept supervision. Manually labeling thousands of images with hundreds of concepts is prohibitively expensive. Our solution exploits a resource that already exists in clinical practice: the free-text radiology reports written alongside each image. We use natural language processing to extract what findings radiologists mentioned in each report and whether those findings were present, absent, or uncertain. This gives us noisy but abundant concept labels at no additional annotation cost.

We describe our approach in two parts. First, we explain how we construct a concept bank from radiology reports and extract per-study concept labels (\S\ref{sec:concept-extraction}). Second, we detail the two-stage training procedure: learning to predict concepts from images, then learning to predict diagnoses from concepts (\S\ref{sec:two-stage-training}).


% =============================================================================
\subsection{Extracting Concept Supervision from Reports}
\label{sec:concept-extraction}
% =============================================================================

Our goal is to convert free-text reports into structured concept labels that can supervise a concept predictor. This involves three steps: extracting mentioned findings from text, linking them to a medical ontology, and aggregating across the dataset to build a concept vocabulary.

\textbf{Extracting findings from text.}
We process each report's findings and impression sections with RadGraph-XL~\cite{jain2021radgraph}, a tool that identifies medical entities and classifies each as \emph{present}, \emph{absent}, or \emph{uncertain}. For example, from the sentence ``No pneumothorax. Mild pulmonary edema is present,'' RadGraph extracts ``pneumothorax'' (absent) and ``pulmonary edema'' (present). We also retain modifier phrases that encode location or laterality, such as ``left lower lobe'' or ``bilateral.''

\textbf{Linking to a standard ontology.}
Raw extracted terms vary in phrasing; ``opacity,'' ``opacification,'' and ``opacities'' might all refer to the same finding. We standardize terminology by linking each extracted mention to the Unified Medical Language System (UMLS)~\cite{bodenreider2004umls}, a comprehensive medical ontology. Specifically, we embed each mention using SapBERT~\cite{liu2021sapbert}, a biomedical language model, and retrieve the nearest UMLS concept based on cosine similarity. We restrict matches to clinically relevant semantic types (diseases, findings, anatomical structures) from SNOMED-CT~\cite{donnelly2006snomed} to avoid spurious links. Mentions that cannot be confidently linked (similarity below 0.8) are discarded.

\textbf{Building the concept vocabulary.}
Aggregating linked concepts across all reports yields thousands of unique findings. We prune this to a manageable vocabulary by keeping only concepts that appear in at least 10 reports with at least one positive mention. We also filter out uninformative normality phrases like ``unremarkable'' or ``no acute findings.'' This produces our final concept bank $\mathcal{C}$ of $K = 1{,}312$ ontology-grounded findings.

\textbf{Handling what reports don't mention.}
A critical subtlety: when a report doesn't mention a finding, we cannot assume that finding is absent, since radiologists only document what they consider relevant. We therefore use \emph{mention masking}: concepts explicitly mentioned in a report receive supervision (positive, negative, or soft target 0.5 for uncertain), while unmentioned concepts are masked and ignored during training. This yields per-study targets $t_{ik} \in \{0, 0.5, 1\}$ and a binary mention mask $m_{ik}$ indicating whether concept $k$ was mentioned in study $i$.

\textbf{Organizing concepts by anatomy.}
Optionally, we group concepts into six coarse anatomical regions $\mathcal{R}$: lung, pleura, heart, mediastinum, bone, and other. We assign each concept $k$ to a parent region $g(k) \in \mathcal{R}$ based on its typical location (derived from modifier phrases in reports) and semantic cues in the concept name. This grouping enables the hierarchical gating mechanism described in \S\ref{sec:region-gating}.


% =============================================================================
\subsection{Two-Stage Concept Bottleneck Training}
\label{sec:two-stage-training}
% =============================================================================

With concept labels extracted from reports, we train \radcbm\ in two stages. Stage~1 learns to predict concepts from images. Stage~2 learns to predict diagnoses from predicted concepts, with the concept predictor frozen. All training and evaluation are performed at the study level using predefined splits, with images, concept labels, and disease labels aligned by study identifier.


% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsubsection{Stage 1: Image to Concepts}
\label{sec:stage1}
% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

A vision encoder $f_\theta$ (we use MedCLIP-pretrained~\cite{wang2022medclip} ViT or ResNet variants) extracts image features $h = f_\theta(x)$, and a two-layer MLP $g_\phi$ predicts concept logits $s = g_\phi(h)$ with probabilities $\hat{c} = \sigma(s)$. Training minimizes binary cross-entropy on concept predictions, but only for concepts explicitly mentioned in the corresponding report:
\begin{equation}
\label{eq:concept_loss}
\mathcal{L}_{\text{concept}} = \frac{1}{\sum_{i,k} m_{ik}} \sum_{i,k} m_{ik} \cdot \mathrm{BCE}(s_{ik}, t_{ik}).
\end{equation}

We evaluate concept prediction quality using micro-averaged AUROC and AUPRC computed over mentioned concepts.


% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsubsection{Stage 2: Concepts to Diagnoses}
\label{sec:stage2}
% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

After Stage~1 converges, we freeze the concept predictor and train a diagnosis head on the predicted concept probabilities. For interpretability, we use a bias-free linear layer:
\begin{equation}
\label{eq:label_head}
\ell_j = \sum_k W_{jk} \cdot \hat{c}_k,
\end{equation}
where $\ell_j$ is the logit for diagnosis $j$. This design makes explanations \emph{intrinsic}: the contribution of concept $k$ to diagnosis $j$ is simply $W_{jk} \cdot \hat{c}_k$. Positive weights indicate supportive concepts; negative weights indicate contradictory ones. Removing the bias ensures the model cannot predict diagnoses without supporting concept evidence. We do not include a residual image-to-label path, so all predictions flow through the bottleneck.

We train with binary cross-entropy on CheXpert-style labels $y_i \in \{0, 1, -1\}^L$, where $-1$ denotes uncertainty. By default, we treat uncertain labels as missing and exclude them from the loss; we also report sensitivity analyses mapping uncertainty to positive or negative.

\textbf{Why freeze the concept predictor?}
Training end-to-end would let the model learn concept representations optimized for diagnosis accuracy rather than concept semantics. Freezing ensures that concept predictions retain their intended clinical meaning and that diagnostic performance reflects the quality of the concept bottleneck itself.


% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsubsection{Hierarchical Region Gating (Optional)}
\label{sec:region-gating}
% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

When anatomical groupings are enabled, we additionally predict region-level gates that indicate whether each anatomical region contains abnormalities. From concept probabilities, we compute region logits $u = W_r \hat{c}$ and convert them to gate probabilities:
\begin{equation}
\label{eq:region_gates}
\hat{z} = \epsilon + (1 - \epsilon) \cdot \sigma\!\left(\frac{u}{\tau}\right),
\end{equation}
where $\tau$ is a temperature parameter and $\epsilon$ is an optional floor that prevents gates from fully closing. Concept predictions are then multiplied by their parent region's gate:
\begin{equation}
\label{eq:gated_concepts}
\hat{c}^{\,\text{gated}}_k = \hat{z}_{g(k)} \cdot \hat{c}_k.
\end{equation}
This prevents anatomically implausible predictions; for instance, a pleural finding cannot contribute to diagnosis if the pleura gate is inactive.

Region gates are supervised using report-derived targets obtained by max-pooling concept targets within each region: $\tilde{z}_r = \max_{k: g(k) = r} t_k$. Regions with no mentioned concepts are masked from the region loss. The gated diagnosis head replaces Eq.~\eqref{eq:label_head} with:
\begin{equation}
\label{eq:gated_label_head}
\ell_j = \sum_k W_{jk} \cdot \hat{c}^{\,\text{gated}}_k.
\end{equation}

The full training objective combines label loss with optional region regularization:
\begin{equation}
\label{eq:full_loss}
\mathcal{L} = \mathcal{L}_{\text{label}} + \lambda_r \mathcal{L}_{\text{region}}.
\end{equation}


% =============================================================================
\subsection{Interpretability by Design}
\label{sec:interpretability}
% =============================================================================

\radcbm's architecture enables several forms of interpretability analysis without post-hoc explanation methods.

\textbf{Concept-level explanations.}
For any prediction, we can list the activated concepts (those with $\hat{c}_k$ above a threshold $\delta$) and their contributions $W_{jk} \cdot \hat{c}^{\,\text{gated}}_k$ to each diagnosis. This provides a complete, human-readable explanation grounded in medical terminology.

\textbf{Concept interventions.}
Because diagnoses depend only on concept probabilities, we can test counterfactuals by editing concept values and observing how predictions change. Setting $\hat{c}_k = 0$ simulates ``what if this finding were absent?'' Setting $\hat{c}_k = 1$ simulates ``what if this finding were definitely present?'' For the linear diagnosis head, the predicted change $\Delta \ell^{\text{pred}}_{jk} = W_{jk} \cdot (\tilde{c}_k - \hat{c}_k)$ exactly matches the observed change, and interventions are \emph{faithful} by construction. We report Pearson correlation between predicted and observed logit changes across interventions.

\textbf{Anatomical plausibility.}
We measure how often the model makes anatomically inconsistent predictions by counting \emph{implausible activations}: cases where $\hat{c}_k > \delta$ but $\hat{z}_{g(k)} < \delta$. Low rates indicate that hierarchical gating successfully constrains predictions to anatomically coherent patterns.

\textbf{Region consistency.}
We quantify alignment between coarse region gates and fine concept evidence using:
\begin{equation}
\label{eq:region_consistency}
\mathrm{RC} = 1 - \frac{1}{N|\mathcal{R}|} \sum_{i=1}^{N} \sum_{r \in \mathcal{R}} \left| \hat{z}_{ir} - \max_{k: g(k)=r} \hat{c}_{ik} \right|.
\end{equation}
Values near 1 indicate that region gates faithfully summarize the underlying concept activations.
