% =============================================================================
% RadCBM Results Section (Revised)
% =============================================================================

\newcommand{\placeholder}[1]{\textbf{#1}}

\subsection{Experimental Setup}

\subsubsection{Datasets}
We evaluate on five chest radiograph benchmarks spanning in-domain and external validation. \textbf{MIMIC-CXR}~\cite{johnson2019mimiccxr} contains 377,110 radiographs from 65,379 patients with associated radiology reports; we use the official train/validation/test splits stratified by patient. \textbf{CheXpert Plus} builds on CheXpert~\cite{irvin2019chexpert}; we evaluate on the radiologist-labeled expert subset. \textbf{VinDr-CXR}~\cite{nguyen2021vindrcxr} provides radiologist annotations for 28 findings, \textbf{RSNA Pneumonia}~\cite{rsna2018pneumonia} provides pneumonia detection labels with bounding boxes, and \textbf{NIH ChestX-ray14}~\cite{wang2017chestxray14} provides 14 disease labels mined from reports.

CheXpert Plus (expert subset), VinDr-CXR, and RSNA Pneumonia provide radiologist-annotated evaluation labels, while NIH ChestX-ray14 labels are report-derived. We emphasize performance on radiologist-labeled subsets as primary evidence of clinical correctness and treat report-derived targets as complementary large-scale evidence.

\subsubsection{Concept Bank Construction}
We extract concepts exclusively from MIMIC-CXR training reports using RadGraph~\cite{jain2021radgraph}, yielding 23,452 unique observation-anatomy pairs. After UMLS normalization, semantic type filtering, and frequency thresholding (minimum 50 occurrences), the final vocabulary contains 1,312 region-specific concepts organized into six anatomical regions: lung (259 concepts), heart (143 concepts), pleura (69 concepts), mediastinum (116 concepts), bone (144 concepts), and other (581 concepts). Assertion status (present, absent, uncertain) is preserved for each concept mention.

\subsubsection{Implementation Details}
We implement \radcbm\ with a frozen radiology-pretrained MedCLIP vision backbone~\cite{wang2022medclip} (Swin-T). Unless stated otherwise, all CBM comparisons in this section use the same MedCLIP transformer backbone (Swin-T) frozen to isolate bottleneck design effects. We additionally report black-box VLM baselines using CXR-CLIP~\cite{you2023cxrclip} and CheXzero~\cite{tiu2022chexzero}. Images are resized to the backbone's native resolution and normalized accordingly. We apply standard augmentations during training: random horizontal flipping, rotation ($\pm 10^{\circ}$), and color jittering.

Models are trained using Adam~\cite{kingma2015adam} with learning rate $10^{-4}$, batch size 32, and early stopping based on validation macro AUC (patience 10 epochs). We set region loss weight $\lambda_r = 1.0$, temperature $\tau = 1.0$, and gate floor $\epsilon = 0.0$ (no clamping); sensitivity analyses are in the supplement. All experiments were conducted on an NVIDIA GeForce RTX 3080 GPU (16GB). We report results averaged over 3 random seeds.

\subsubsection{Baselines and Comparison Protocol}
We compare against concept bottleneck models (CBMs) and black-box baselines.

\textbf{CBM baselines.} (1)~\textbf{Post-hoc CBM}~\cite{yuksekgonul2023posthoc}, which retrofits concept bottlenecks onto pretrained models; \textbf{LaBo}~\cite{yang2023labo}, which constructs text-defined bottlenecks with linear concept-to-class predictors (Following LaBo's prompts and 500-sentences/class budget, we use OpenAI \texttt{gpt-3.5-turbo} to generate candidate sentences and extract short concepts; this substitutes for LaBo's deprecated GPT-3 generator and unreleased fine-tuned T5 extractor, and we keep LaBo's cleaning heuristics unchanged); (3)~\textbf{AdaCBM}~\cite{xu2024adacbm}, which adds an adaptive module to reduce domain mismatch; and (4)~\textbf{C2F-CBM}~\cite{panousis2024coarsetofineconceptbottleneckmodels}, which builds two-level bottlenecks with coarse-to-fine prediction.

\textbf{Black-box baselines.} Supervised CNNs (ResNet-50, DenseNet-121)~\cite{cohen2022xrv} and vision-language models used as black-box encoders (MedCLIP, CXR-CLIP, CheXzero).

\textbf{Comparison protocol.} To ensure fair comparison, we follow recent recommendations for evaluating VLM-CBMs~\cite{debole2025ifconcept}:
\begin{itemize}[nosep,leftmargin=*]
    \item All CBMs within a comparison use the same frozen vision backbone.
    \item For concept-level evaluation (Table~\ref{tab:concept_quality}), all methods are evaluated on the same 1,312-concept target set. For LaBo, we use an ontology-aligned variant (``LaBo (fixed vocab)'') that takes our concept bank as input.
    \item Hyperparameters are tuned per method via validation-based early stopping with a fixed search budget.
    \item Train/validation/test splits and uncertainty handling are identical across methods.
\end{itemize}
All supervised baselines and CBMs are trained on MIMIC-CXR train/val and evaluated on MIMIC-CXR test and external datasets without retraining; for RSNA (binary pneumonia), we fit a dataset-specific pneumonia head when the method lacks that label. Zero-shot VLMs are evaluated directly on each test set.
For label-level evaluation (Tables~\ref{tab:classification},~\ref{tab:external_validation}), each CBM method uses its native concept source, as the concept bank is part of the method's contribution. For interpretability metrics (Table~\ref{tab:interpretability}), we evaluate only intrinsic CBMs where the bottleneck mediates predictions; post-hoc CBMs are excluded since concept interventions do not affect their underlying predictor.

\subsubsection{Evaluation Metrics}
\textbf{Classification performance} is reported using per-label and macro-averaged AUC-ROC on the five CheXpert competition labels (Atelectasis, Cardiomegaly, Consolidation, Edema, Pleural Effusion). Full 14-label results are in the supplement. When thresholded metrics are reported, per-label thresholds are tuned on MIMIC-CXR validation and fixed for all test sets.

\textbf{Concept quality} is assessed on the shared 1,312-concept bank using macro AUC-ROC and macro AUPRC, reported overall and on rare concepts (50--200 training occurrences). We compute concept metrics only on explicitly asserted mentions; unmentioned concepts are unlabeled. Macro averages include only concepts with at least one labeled positive and one labeled negative example.

\textbf{Interpretability} is evaluated via three metrics:
(1)~\emph{Intervention faithfulness}: Pearson correlation between predicted concept contribution ($W_{jk} \cdot \hat{c}^{\text{gated}}_k$) and observed label change upon setting $\hat{c}_k$ to 0 or 1. For the linear head, this correlation is 1.0 by construction.
(2)~\emph{Plausibility}: fraction of activated findings ($\hat{c}_k > 0.5$) whose parent region gate exceeds 0.5.
(3)~\emph{Implausible activation rate}: fraction of finding activations occurring when the parent region gate is below 0.3.
(4)~\emph{Region consistency}: agreement between region gates and max-pooled concept activations (Eq.~\ref{eq:region_consistency} in supplement).

% =============================================================================
% TABLE 1: Main Classification Results
% =============================================================================

\subsection{Classification Performance}

Table~\ref{tab:classification} presents classification performance on the five CheXpert competition labels. \radcbm\ is competitive with CBM baselines while providing interpretable concept-mediated predictions. On MIMIC-CXR, \radcbm\ achieves a macro AUC of 0.794 $\pm$ 0.001, outperforming intrinsic CBMs (LaBo, AdaCBM) and approaching the post-hoc CBM, while remaining competitive with the supervised DenseNet-121 baseline (0.684). Hierarchical gating yields a small macro AUC gain over the flat variant on MIMIC-CXR (0.794 vs.\ 0.788), with negligible per-label differences ($\leq$0.001) for Pleural Effusion and Edema.

Among CBM approaches, methods relying on small or automatically generated concept vocabularies exhibit lower classification performance, suggesting that ontology-grounded concept banks with broader coverage provide stronger supervisory signal.

\begin{table}[htbp]
\centering
\caption{Classification performance (AUC-ROC) on MIMIC-CXR test set and CheXpert Plus expert subset for five competition labels. Best in \textbf{bold}, second-best \underline{underlined}. All concept-based methods share the same frozen backbone (MedCLIP Swin-T). Results are mean $\pm$ std over 3 seeds for seeded methods; single-run baselines are shown without std.}
\label{tab:classification}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llccccc|c}
\toprule
\textbf{Method} & \textbf{Type} & \rotatebox{90}{Atelect.} & \rotatebox{90}{Cardiom.} & \rotatebox{90}{Consolid.} & \rotatebox{90}{Edema} & \rotatebox{90}{Pl. Eff.} & \textbf{Macro} \\
\midrule
\multicolumn{8}{l}{\textit{MIMIC-CXR Test Set}} \\
\midrule
ResNet-50 & CNN & 0.66 & 0.71 & 0.67 & 0.76 & 0.80 & 0.721 \\
DenseNet-121 & CNN & 0.62 & 0.70 & 0.59 & 0.75 & 0.75 & 0.684 \\
MedCLIP (Swin-T) & VLM & 0.74 & 0.75 & 0.77 & 0.85 & 0.89 & 0.797 \\
CXR-CLIP (Swin-T) & VLM & 0.45 & 0.58 & 0.52 & 0.64 & 0.51 & 0.539 \\
CheXzero (ViT-B/32) & VLM & 0.65 & 0.72 & 0.70 & 0.82 & 0.84 & 0.749 \\
\midrule
Post-hoc CBM & CBM & 0.77 & 0.77 & 0.72 & 0.86 & 0.90 & 0.803 \\
LaBo & CBM & 0.63 & 0.74 & 0.53 & 0.72 & 0.88 & 0.702 \\
AdaCBM & CBM & 0.74 & 0.78 & 0.64 & 0.83 & 0.89 & 0.775 \\
C2F-CBM & H-CBM & 0.768 $\pm$ 0.005 & 0.744 $\pm$ 0.007 & 0.739 $\pm$ 0.009 & 0.857 $\pm$ 0.002 & 0.876 $\pm$ 0.002 & 0.797 $\pm$ 0.005 \\
\midrule
\radcbm\ (flat) & CBM & 0.76 & 0.75 & 0.72 & 0.85 & 0.87 & 0.788 $\pm$ 0.002 \\
\radcbm\ & H-CBM & 0.76 & 0.75 & 0.74 & 0.85 & 0.87 & 0.794 $\pm$ 0.001 \\
\midrule
\multicolumn{8}{l}{\textit{CheXpert Plus Expert Subset}} \\
\midrule
ResNet-50 & CNN & 0.56 & 0.48 & 0.42 & 0.52 & 0.55 & 0.506 \\
DenseNet-121 & CNN & 0.53 & 0.42 & 0.49 & 0.60 & 0.57 & 0.521 \\
MedCLIP (Swin-T) & VLM & 0.49 & 0.59 & 0.63 & 0.49 & 0.49 & 0.539 \\
CXR-CLIP (Swin-T) & VLM & 0.50 & 0.44 & 0.60 & 0.50 & 0.50 & 0.509 \\
CheXzero (ViT-B/32) & VLM & 0.54 & 0.49 & 0.58 & 0.55 & 0.55 & 0.543 \\
\midrule
Post-hoc CBM & CBM & 0.58 & 0.38 & 0.42 & 0.53 & 0.53 & 0.487 \\
LaBo & CBM & 0.45 & 0.48 & 0.47 & 0.53 & 0.49 & 0.483 \\
AdaCBM & CBM & 0.56 & 0.36 & 0.59 & 0.55 & 0.54 & 0.518 \\
C2F-CBM & H-CBM & 0.589 $\pm$ 0.002 & 0.430 $\pm$ 0.004 & 0.477 $\pm$ 0.008 & 0.590 $\pm$ 0.001 & 0.550 $\pm$ 0.002 & 0.527 $\pm$ 0.002 \\
\midrule
\radcbm\ (flat) & CBM & 0.58 & 0.42 & 0.50 & 0.59 & 0.54 & 0.527 $\pm$ 0.006 \\
\radcbm\ & H-CBM & 0.58 & 0.42 & 0.50 & 0.58 & 0.54 & 0.526 $\pm$ 0.001 \\
\bottomrule
\end{tabular}%
}
\end{table}

% =============================================================================
% TABLE 2: Concept Quality
% =============================================================================

\subsection{Concept Quality}

Table~\ref{tab:concept_quality} compares concept prediction on the shared 1,312-concept target set. We report concept quality only for intrinsic CBMs; post-hoc CBMs are omitted since their concept scores are not trained to match a target concept bank. \radcbm\ achieves the highest overall concept AUC (0.693) and AUPRC (0.861). Rare-concept AUC is lower than the flat variant (0.507 vs.\ 0.641), suggesting gating trades rare sensitivity for stronger overall calibration.

\begin{table}[t]
\centering
\caption{Concept prediction quality on MIMIC-CXR test set (shared 1,312-concept bank). \textsuperscript{\dag}LaBo evaluated with ontology-aligned variant. Results: mean $\pm$ std over 3 seeds when available.}
\label{tab:concept_quality}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{AUC} & \textbf{AUPRC} & \textbf{Rare AUC} & \textbf{Rare AUPRC} \\
\midrule
LaBo\textsuperscript{\dag} & 0.474 $\pm$ 0.000 & 0.778 $\pm$ 0.000 & 0.496 $\pm$ 0.000 & 0.728 $\pm$ 0.000 \\
AdaCBM & 0.622 $\pm$ 0.003 & 0.854 $\pm$ 0.001 & 0.614 $\pm$ 0.011 & 0.781 $\pm$ 0.002 \\
C2F-CBM & 0.542 $\pm$ 0.001 & 0.810 $\pm$ 0.001 & 0.446 $\pm$ 0.018 & 0.668 $\pm$ 0.005 \\
\midrule
\radcbm\ (flat) & 0.517 & 0.804 & 0.641 & 0.782 \\
\radcbm\ & 0.693 & 0.861 & 0.507 & 0.730 \\
\bottomrule
\end{tabular}%
}
\end{table}

% =============================================================================
% TABLE 3: External Validation
% =============================================================================

\subsection{External Validation}

Table~\ref{tab:external_validation} reports generalization to external benchmarks. For multi-label datasets, we report macro AUC over the five CheXpert competition labels using dataset-specific mappings; for RSNA we report binary pneumonia AUC. Per-label thresholds tuned on MIMIC-CXR validation are applied without further tuning. For RSNA, we fit a binary pneumonia head when the method does not include a native pneumonia label.

\radcbm\ generalizes competitively across all benchmarks, with smaller performance drops than black-box baselines on distribution shift (VinDr-CXR, NIH). This suggests that ontology-grounded concepts provide more transferable intermediate representations than end-to-end learned features.

\begin{table}[t]
\centering
\caption{External validation (AUC-ROC). Multi-label columns: macro AUC over five CheXpert labels; RSNA: binary pneumonia AUC. Post-hoc CBM is omitted since its predictions match its underlying black-box model by construction. RSNA values for LaBo and C2F are omitted (---) due to missing pneumonia heads.}
\label{tab:external_validation}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llccccc}
\toprule
\textbf{Method} & \textbf{Type} & \textbf{MIMIC} & \textbf{CheXpert Plus} & \textbf{VinDr-CXR} & \textbf{NIH} & \textbf{RSNA} \\
\midrule
ResNet-50 & CNN & 0.721 & 0.506 & 0.867 & 0.817 & 0.892 \\
DenseNet-121 & CNN & 0.684 & 0.521 & 0.854 & 0.647 & 0.772 \\
MedCLIP & VLM & 0.797 & 0.539 & 0.910 & 0.789 & 0.831 \\
CXR-CLIP (Swin-T) & VLM & 0.539 & 0.509 & 0.841 & 0.744 & 0.779 \\
CheXzero (ViT-B/32) & VLM & 0.749 & 0.543 & 0.870 & 0.765 & 0.839 \\
\midrule
LaBo & CBM & 0.702 & 0.483 & 0.927 & 0.763 & --- \\
AdaCBM & CBM & 0.775 & 0.518 & 0.935 & 0.762 & 0.898 \\
C2F-CBM & H-CBM & 0.797 & 0.527 & 0.926 & 0.765 & --- \\
\midrule
\radcbm\ (flat) & CBM & 0.788 & 0.527 & 0.927 & 0.757 & 0.873 \\
\radcbm\ & H-CBM & 0.794 & 0.526 & 0.927 & 0.761 & 0.873 \\
\bottomrule
\end{tabular}%
}
\end{table}

% =============================================================================
% TABLE 4: Interpretability
% =============================================================================

\subsection{Interpretability}

Table~\ref{tab:interpretability} evaluates whether concept-based explanations support faithful interventions and clinically plausible activations. We report metrics only for intrinsic CBMs where the bottleneck mediates label predictions.

\radcbm\ achieves perfect intervention faithfulness by construction (the linear head ensures predicted and observed effects match exactly). Hierarchical gating reduces the implausible activation rate from 0.11\% (flat) to 0.00\%, indicating that region gates successfully suppress findings in anatomically inactive regions. Region consistency (0.847) confirms that gates align with pooled concept evidence.

\begin{table}[t]
\centering
\caption{Interpretability metrics on MIMIC-CXR (intrinsic CBMs only). \textsuperscript{\dag}LaBo evaluated with ontology-aligned variant. Results: mean $\pm$ std over 3 seeds.}
\label{tab:interpretability}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{Interv.} & \textbf{Plaus.} & \textbf{Implaus.} & \textbf{Region} \\
 & \textbf{Faith.} $\uparrow$ & $\uparrow$ & \textbf{Rate} $\downarrow$ & \textbf{Cons.} $\uparrow$ \\
\midrule
LaBo\textsuperscript{\dag} & 0.752 $\pm$ 0.033 & 1.000 $\pm$ 0.000 & 0.000 $\pm$ 0.000 & 0.511 $\pm$ 0.005 \\
AdaCBM & 0.864 $\pm$ 0.015 & 1.000 $\pm$ 0.000 & 0.000 $\pm$ 0.000 & 0.531 $\pm$ 0.016 \\
C2F-CBM & 0.997 $\pm$ 0.001 & 0.981 $\pm$ 0.002 & 0.017 $\pm$ 0.002 & --- \\
\midrule
\radcbm\ (flat) & 0.999 $\pm$ 0.000 & 0.987 $\pm$ 0.000 & 0.001 $\pm$ 0.000 & --- \\
\radcbm\ & 1.000 $\pm$ 0.000 & 1.000 $\pm$ 0.000 & 0.000 $\pm$ 0.000 & 0.847 $\pm$ 0.026 \\
\bottomrule
\end{tabular}
\end{table}

% =============================================================================
% Ablations
% =============================================================================

\subsection{Ablation Study}

We report ablations isolating key design choices in the supplement (Table~\ref{tab:ablation}). Briefly, mention masking and assertion-aware targets improve concept quality by reducing label noise, hierarchical gating improves plausibility with minimal impact on classification AUC, and conservative soft-gating ($\epsilon > 0$) mitigates cascading failures where missed region predictions suppress all constituent findings.
