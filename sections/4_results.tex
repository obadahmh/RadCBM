% =============================================================================
% RadCBM Results Section (Revised)
% =============================================================================

\newcommand{\placeholder}[1]{\textbf{#1}}

\subsection{Experimental Setup}

\subsubsection{Datasets}
We evaluate on five chest radiograph benchmarks spanning in-domain and external validation. \textbf{MIMIC-CXR}~\cite{johnson2019mimiccxr} contains \placeholder{377,110} radiographs from \placeholder{65,379} patients with associated radiology reports; we use the official train/validation/test splits stratified by patient. \textbf{CheXpert Plus} builds on CheXpert~\cite{irvin2019chexpert}; we evaluate on the radiologist-labeled expert subset. \textbf{VinDr-CXR}~\cite{nguyen2021vindrcxr} provides radiologist annotations for 28 findings, \textbf{RSNA Pneumonia}~\cite{rsna2018pneumonia} provides pneumonia detection labels with bounding boxes, and \textbf{NIH ChestX-ray14}~\cite{wang2017chestxray14} provides 14 disease labels mined from reports.

CheXpert Plus (expert subset), VinDr-CXR, and RSNA Pneumonia provide radiologist-annotated evaluation labels, while NIH ChestX-ray14 labels are report-derived. We emphasize performance on radiologist-labeled subsets as primary evidence of clinical correctness and treat report-derived targets as complementary large-scale evidence.

\subsubsection{Concept Bank Construction}
We extract concepts exclusively from MIMIC-CXR training reports using RadGraph~\cite{jain2021radgraph}, yielding \placeholder{127,834} unique observation-anatomy pairs. After UMLS normalization, semantic type filtering, and frequency thresholding (minimum 50 occurrences), the final vocabulary contains 1,312 region-specific concepts organized into six anatomical regions: lung (\placeholder{XXX} concepts), heart (\placeholder{XXX} concepts), pleura (\placeholder{XXX} concepts), mediastinum (\placeholder{XXX} concepts), bone (\placeholder{XXX} concepts), and other (\placeholder{XXX} concepts). Assertion status (present, absent, uncertain) is preserved for each concept mention.

\subsubsection{Implementation Details}
We implement \radcbm\ with a frozen radiology-pretrained MedCLIP vision backbone~\cite{wang2022medclip} (Swin-T). Unless stated otherwise, all CBM comparisons in this section use the same MedCLIP transformer backbone (Swin-T) frozen to isolate bottleneck design effects. We additionally report black-box VLM baselines using CXR-CLIP~\cite{you2023cxrclip} and CheXzero~\cite{tiu2022chexzero}. Images are resized to the backbone's native resolution and normalized accordingly. We apply standard augmentations during training: random horizontal flipping, rotation ($\pm 10^{\circ}$), and color jittering.

Models are trained using Adam~\cite{kingma2015adam} with learning rate $10^{-4}$, batch size 32, and early stopping based on validation macro AUC (patience 10 epochs). We set region loss weight $\lambda_r = 0.1$, temperature $\tau = 1.0$, and gate floor $\epsilon = 0.01$ based on validation performance; sensitivity analyses are in the supplement. All experiments were conducted on an NVIDIA GeForce RTX 3080 GPU (16GB). We report results averaged over 3 random seeds.

\subsubsection{Baselines and Comparison Protocol}
We compare against concept bottleneck models (CBMs) and black-box baselines.

\textbf{CBM baselines.} (1)~\textbf{Post-hoc CBM}~\cite{yuksekgonul2023posthoc}, which retrofits concept bottlenecks onto pretrained models; \textbf{LaBo}~\cite{yang2023labo}, which constructs text-defined bottlenecks with linear concept-to-class predictors (Following LaBo's prompts and 500-sentences/class budget, we use OpenAI \texttt{gpt-3.5-turbo} to generate candidate sentences and extract short concepts; this substitutes for LaBo's deprecated GPT-3 generator and unreleased fine-tuned T5 extractor, and we keep LaBo's cleaning heuristics unchanged); (3)~\textbf{AdaCBM}~\cite{xu2024adacbm}, which adds an adaptive module to reduce domain mismatch; and (4)~\textbf{C2F-CBM}~\cite{panousis2024coarsetofineconceptbottleneckmodels}, which builds two-level bottlenecks with coarse-to-fine prediction.

\textbf{Black-box baselines.} Supervised CNNs (ResNet-50, DenseNet-121)~\cite{cohen2022xrv} and vision-language models used as black-box encoders (MedCLIP, CXR-CLIP, CheXzero).

\textbf{Comparison protocol.} To ensure fair comparison, we follow recent recommendations for evaluating VLM-CBMs~\cite{debole2025ifconcept}:
\begin{itemize}[nosep,leftmargin=*]
    \item All CBMs within a comparison use the same frozen vision backbone.
    \item For concept-level evaluation (Table~\ref{tab:concept_quality}), all methods are evaluated on the same 1,312-concept target set. For LaBo, we use an ontology-aligned variant (``LaBo (fixed vocab)'') that takes our concept bank as input.
    \item Hyperparameters are tuned per method via validation-based early stopping with a fixed search budget.
    \item Train/validation/test splits and uncertainty handling are identical across methods.
\end{itemize}
For label-level evaluation (Tables~\ref{tab:classification},~\ref{tab:external_validation}), each CBM method uses its native concept source, as the concept bank is part of the method's contribution. For interpretability metrics (Table~\ref{tab:interpretability}), we evaluate only intrinsic CBMs where the bottleneck mediates predictions; post-hoc CBMs are excluded since concept interventions do not affect their underlying predictor.

\subsubsection{Evaluation Metrics}
\textbf{Classification performance} is reported using per-label and macro-averaged AUC-ROC on the five CheXpert competition labels (Atelectasis, Cardiomegaly, Consolidation, Edema, Pleural Effusion). Full 14-label results are in the supplement. When thresholded metrics are reported, per-label thresholds are tuned on MIMIC-CXR validation and fixed for all test sets.

\textbf{Concept quality} is assessed on the shared 1,312-concept bank using macro AUC-ROC and macro AUPRC, reported overall and on rare concepts (50--200 training occurrences).

\textbf{Interpretability} is evaluated via three metrics:
(1)~\emph{Intervention faithfulness}: Pearson correlation between predicted concept contribution ($W_{jk} \cdot \hat{c}^{\text{gated}}_k$) and observed label change upon setting $\hat{c}_k$ to 0 or 1. For the linear head, this correlation is 1.0 by construction.
(2)~\emph{Plausibility}: fraction of activated findings ($\hat{c}_k > 0.5$) whose parent region gate exceeds 0.5.
(3)~\emph{Implausible activation rate}: fraction of finding activations occurring when the parent region gate is below 0.3.
(4)~\emph{Region consistency}: agreement between region gates and max-pooled concept activations (Eq.~\ref{eq:region_consistency} in supplement).

% =============================================================================
% TABLE 1: Main Classification Results
% =============================================================================

\subsection{Classification Performance}

Table~\ref{tab:classification} presents classification performance on the five CheXpert competition labels. \radcbm\ matches or exceeds all CBM baselines while providing interpretable concept-mediated predictions. On MIMIC-CXR, \radcbm\ achieves a macro AUC of \placeholder{0.XXX}, comparable to the supervised DenseNet-121 baseline (\placeholder{0.684}) and outperforming all other CBM methods. Hierarchical gating improves over the flat variant by \placeholder{X.X} points in macro AUC, with notable gains on region-specific pathologies such as Pleural Effusion (\placeholder{+X.X}) and Edema (\placeholder{+X.X}).

Among CBM approaches, methods relying on small or automatically generated concept vocabularies exhibit lower classification performance, suggesting that ontology-grounded concept banks with broader coverage provide stronger supervisory signal.

\begin{table}[htbp]
\centering
\caption{Classification performance (AUC-ROC) on MIMIC-CXR test set and CheXpert Plus expert subset for five competition labels. Best in \textbf{bold}, second-best \underline{underlined}. All concept-based methods share the same frozen backbone (MedCLIP Swin-T). Results are mean over 3 seeds; std $<$0.01 omitted.}
\label{tab:classification}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llccccc|c}
\toprule
\textbf{Method} & \textbf{Type} & \rotatebox{90}{Atelect.} & \rotatebox{90}{Cardiom.} & \rotatebox{90}{Consolid.} & \rotatebox{90}{Edema} & \rotatebox{90}{Pl. Eff.} & \textbf{Macro} \\
\midrule
\multicolumn{8}{l}{\textit{MIMIC-CXR Test Set}} \\
\midrule
ResNet-50 & CNN & 0.66 & 0.71 & 0.67 & 0.76 & 0.80 & 0.721 \\
DenseNet-121 & CNN & 0.62 & 0.70 & 0.59 & 0.75 & 0.75 & 0.684 \\
MedCLIP (Swin-T) & VLM & 0.74 & 0.75 & 0.77 & 0.85 & 0.89 & 0.797 \\
CXR-CLIP (Swin-T) & VLM & 0.45 & 0.58 & 0.52 & 0.64 & 0.51 & 0.539 \\
CheXzero (ViT-B/32) & VLM & 0.65 & 0.72 & 0.70 & 0.82 & 0.84 & 0.749 \\
\midrule
Post-hoc CBM & CBM & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XXX} \\
LaBo & CBM & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XXX} \\
AdaCBM & CBM & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XXX} \\
C2F-CBM & H-CBM & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XXX} \\
\midrule
\radcbm\ (flat) & CBM & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XXX} \\
\radcbm\ & H-CBM & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XXX} \\
\midrule
\multicolumn{8}{l}{\textit{CheXpert Plus Expert Subset}} \\
\midrule
ResNet-50 & CNN & 0.56 & 0.48 & 0.42 & 0.52 & 0.55 & 0.506 \\
DenseNet-121 & CNN & 0.53 & 0.42 & 0.49 & 0.60 & 0.57 & 0.521 \\
MedCLIP (Swin-T) & VLM & 0.49 & 0.59 & 0.63 & 0.49 & 0.49 & 0.539 \\
CXR-CLIP (Swin-T) & VLM & 0.50 & 0.44 & 0.60 & 0.50 & 0.50 & 0.509 \\
CheXzero (ViT-B/32) & VLM & 0.54 & 0.49 & 0.58 & 0.55 & 0.55 & 0.543 \\
\midrule
Post-hoc CBM & CBM & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XXX} \\
LaBo & CBM & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XXX} \\
AdaCBM & CBM & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XXX} \\
C2F-CBM & H-CBM & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XXX} \\
\midrule
\radcbm\ (flat) & CBM & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XXX} \\
\radcbm\ & H-CBM & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XX} & \placeholder{.XXX} \\
\bottomrule
\end{tabular}%
}
\end{table}

% =============================================================================
% TABLE 2: Concept Quality
% =============================================================================

\subsection{Concept Quality}

Table~\ref{tab:concept_quality} compares concept prediction on the shared 1,312-concept target set. \radcbm\ achieves the highest overall concept AUC (\placeholder{0.XXX}) and AUPRC (\placeholder{0.XXX}). The improvement is pronounced for rare concepts: \radcbm\ attains \placeholder{0.XXX} AUC on rare findings compared to \placeholder{0.XXX} for the flat variant, consistent with hierarchical gating suppressing spurious activations when regions are predicted normal.

\begin{table}[t]
\centering
\caption{Concept prediction quality on MIMIC-CXR test set (shared 1,312-concept bank). \textsuperscript{\dag}LaBo evaluated with ontology-aligned variant. Results: mean $\pm$ std over 3 seeds.}
\label{tab:concept_quality}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{AUC} & \textbf{AUPRC} & \textbf{Rare AUC} & \textbf{Rare AUPRC} \\
\midrule
Post-hoc CBM & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} \\
LaBo\textsuperscript{\dag} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} \\
AdaCBM & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} \\
C2F-CBM & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} \\
\midrule
\radcbm\ (flat) & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} \\
\radcbm\ & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} \\
\bottomrule
\end{tabular}
\end{table}

% =============================================================================
% TABLE 3: External Validation
% =============================================================================

\subsection{External Validation}

Table~\ref{tab:external_validation} reports generalization to external benchmarks. For multi-label datasets, we report macro AUC over the five CheXpert competition labels using dataset-specific mappings; for RSNA we report binary pneumonia AUC. Per-label thresholds tuned on MIMIC-CXR validation are applied without further tuning.

\radcbm\ generalizes competitively across all benchmarks, with smaller performance drops than black-box baselines on distribution shift (VinDr-CXR, NIH). This suggests that ontology-grounded concepts provide more transferable intermediate representations than end-to-end learned features.

\begin{table}[t]
\centering
\caption{External validation (AUC-ROC). Multi-label columns: macro AUC over five CheXpert labels; RSNA: binary pneumonia AUC. Post-hoc CBM is omitted since its predictions match its underlying black-box model by construction.}
\label{tab:external_validation}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llccccc}
\toprule
\textbf{Method} & \textbf{Type} & \textbf{MIMIC} & \textbf{CheXpert Plus} & \textbf{VinDr-CXR} & \textbf{NIH} & \textbf{RSNA} \\
\midrule
DenseNet-121 & CNN & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} \\
MedCLIP & VLM & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} \\
\midrule
LaBo & CBM & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} \\
AdaCBM & CBM & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} \\
C2F-CBM & H-CBM & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} \\
\midrule
\radcbm\ (flat) & CBM & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} \\
\radcbm\ & H-CBM & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} \\
\bottomrule
\end{tabular}%
}
\end{table}

% =============================================================================
% TABLE 4: Interpretability
% =============================================================================

\subsection{Interpretability}

Table~\ref{tab:interpretability} evaluates whether concept-based explanations support faithful interventions and clinically plausible activations. We report metrics only for intrinsic CBMs where the bottleneck mediates label predictions.

\radcbm\ achieves perfect intervention faithfulness by construction (the linear head ensures predicted and observed effects match exactly). Hierarchical gating reduces the implausible activation rate from \placeholder{XX.X\%} (flat) to \placeholder{XX.X\%}, indicating that region gates successfully suppress findings in anatomically inactive regions. Region consistency (\placeholder{0.XXX}) confirms that gates align with pooled concept evidence.

\begin{table}[t]
\centering
\caption{Interpretability metrics on MIMIC-CXR (intrinsic CBMs only). \textsuperscript{\dag}LaBo evaluated with ontology-aligned variant. Results: mean $\pm$ std over 3 seeds.}
\label{tab:interpretability}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{Interv.} & \textbf{Plaus.} & \textbf{Implaus.} & \textbf{Region} \\
 & \textbf{Faith.} $\uparrow$ & $\uparrow$ & \textbf{Rate} $\downarrow$ & \textbf{Cons.} $\uparrow$ \\
\midrule
LaBo\textsuperscript{\dag} & \placeholder{.XX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} \\
AdaCBM & \placeholder{.XX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} \\
C2F-CBM & \placeholder{.XX} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} \\
\midrule
\radcbm\ (flat) & \placeholder{.XX} & \placeholder{.XXX} & \placeholder{.XXX} & --- \\
\radcbm\ & \placeholder{1.00} & \placeholder{.XXX} & \placeholder{.XXX} & \placeholder{.XXX} \\
\bottomrule
\end{tabular}
\end{table}

% =============================================================================
% Ablations
% =============================================================================

\subsection{Ablation Study}

We report ablations isolating key design choices in the supplement (Table~\ref{tab:ablation}). Briefly: (1) assertion-aware mention masking improves concept AUC by \placeholder{X.X} points by avoiding supervision corruption from negated findings; (2) hierarchical gating improves plausibility from \placeholder{XX\%} to \placeholder{XX\%} with minimal impact on classification AUC; (3) conservative soft-gating ($\epsilon > 0$) prevents cascading failures where missed region predictions suppress all constituent findings.
