Chest radiography remains the most frequently performed imaging examination worldwide, with hundreds of millions of studies acquired annually~\cite{raoof2012interpretation}. Interpreting these images is high-stakes: a missed pneumothorax, an overlooked nodule, or a mischaracterized cardiac silhouette can alter the trajectory of patient care~\cite{bruno2015understanding}. Because the sheer volume of studies strains radiology workflows, diagnostic errors (while individually rare) accumulate into a substantial burden when multiplied across populations~\cite{brady2017error,donald2014common}. The promise of computational assistance is therefore not merely academic. Systems that can reliably flag abnormalities, prioritize urgent cases, or provide differential considerations address a genuine clinical need~\cite{litjens2017survey}.

Deep learning has delivered remarkable progress toward this goal. Convolutional and transformer-based architectures now match or exceed physician-level performance on curated benchmarks for thoracic pathology detection~\cite{rajpurkar2017chexnet,irvin2019chexpert,singh2024efficient,shamshad2023transformers}. These results, however, have not translated proportionally into clinical deployment~\cite{kelly2019key,nagendran2020artificial}. Part of this gap reflects concerns about robustness and generalization: models that perform well on internal test sets can fail when applied to external hospitals, sometimes because they exploit institution-specific artifacts rather than disease-related signal~\cite{zech2018confounding}. Reasons also include regulatory, infrastructural, and cultural barriers~\cite{kelly2019key}.

One obstacle recurs with particular frequency in conversations with radiologists: the predictions are unaccompanied by reasoning~\cite{rudin2019stop}. A model may assert ``cardiomegaly'' with high confidence, but it cannot articulate \emph{why} or point to the cardiothoracic ratio it implicitly computed. It cannot situate its judgment within the anatomical and pathophysiological logic that governs clinical interpretation. This is not mere aesthetic preference for explanation. Radiologists think in concepts such as consolidation, air bronchograms, Kerley lines, and costophrenic blunting, and a system that cannot speak this language offers predictions without a basis for trust or correction~\cite{reyes2020interpretability}.

Concept-based models, often instantiated as Concept Bottleneck Models (CBMs), offer an architectural response to this limitation~\cite{koh2020concept}. Instead of mapping pixels directly to diagnostic labels, they introduce an intermediate representation of human-interpretable attributes. The model first predicts whether specific concepts are present (anatomical structures, radiographic findings, device positions) and then uses those concepts to produce diagnostic outputs. Explanations are thus part of the forward pass rather than added post hoc through saliency methods~\cite{selvaraju2017gradcam}. When a CBM predicts pulmonary edema, we can inspect whether it detected cardiomegaly, vascular redistribution, or interstitial opacities and check that this reasoning aligns with clinical knowledge.

This interpretability, however, comes at a cost that has limited practical adoption: concept-based models require concepts. Specifically, they require a predefined vocabulary of clinically meaningful attributes and, more demandingly, supervisory signal indicating which concepts are present in which images. Manual annotation at this granularity is expensive, time-consuming, and difficult to scale~\cite{willemink2020preparing}. A single chest radiograph might exhibit dozens of relevant findings across multiple anatomical regions, each requiring expert assessment. Curated ontologies such as SNOMED CT provide standardized vocabularies~\cite{donnelly2006snomed}, but these resources define \emph{what} concepts exist, not \emph{where} they appear in any particular image. The gap between the conceptual richness that would make these models clinically useful and the annotation budgets that real projects can sustain has constrained concept-based approaches to modest scales or narrow concept sets~\cite{oikarinen2023labelfree}.

In this work we propose \radcbm, a hierarchical CBM for chest X-ray interpretation that replaces manual per-image concept labels with concept targets mined automatically from paired radiology reports. \radcbm\ organizes concepts into an anatomy-first hierarchy, predicts region abnormality and region-specific findings, and constrains diagnostic labels to be linear functions of these gated concepts, making its reasoning explicit by construction.

To supervise \radcbm\ at scale, we exploit supervision that already exists in clinical practice but is stored in a different form. Every radiology report is, in effect, a concept annotation. When a radiologist dictates ``bilateral lower lobe consolidations with air bronchograms, consistent with pneumonia,'' they have identified anatomical locations, described radiographic findings, and linked observations to a diagnostic impression. This information is recorded in natural language rather than structured labels, but it is expert-generated, temporally aligned with the image, and available at scale in virtually every institution with an electronic health record~\cite{johnson2019mimiccxr}. The challenge is to turn this free text into supervision suitable for training concept-based vision models.

Transforming free-text reports into structured concept representations is not straightforward. Radiology language is dense with abbreviations, implicit negations, and context-dependent qualifications~\cite{denny2009extracting}. A finding may be ``present,'' ``absent,'' ``unchanged,'' or ``cannot be excluded,'' distinctions that matter clinically and must be preserved in any derived supervision~\cite{chapman2001negex,smit2020chexbert}. Linking extracted mentions to standardized terminologies introduces additional complexity: the same concept may be expressed in myriad surface forms, and disambiguation requires domain-specific knowledge. Recent advances in clinical natural language processing and biomedical entity linking~\cite{liu2021sapbert,jain2021radgraph}, together with resources such as the Unified Medical Language System (UMLS)~\cite{bodenreider2004umls}, make such extraction increasingly tractable. However, these tools have rarely been combined into pipelines that produce \emph{trainable} concept banks with assertion status, anatomical context, and ontological grounding.

This work addresses the gap between the latent supervision encoded in radiology reports and the structured representations that concept-based vision models require. Prior efforts have tackled adjacent problems: extracting findings from clinical text~\cite{peng2018negbio,irvin2019chexpert}, linking medical entities to ontologies such as UMLS~\cite{bodenreider2004umls}, and training interpretable classifiers on manually curated concept sets~\cite{koh2020concept}. These efforts, however, stop short of turning large report corpora into trainable, ontology-grounded concept banks and pairing them with hierarchical CBMs for chest radiography. Unlike systems that use reports primarily to derive noisy image-level labels for black-box classifiers or that restrict CBMs to small, hand-designed concept sets, we convert routine report corpora into ontology-grounded concept banks and use them to supervise \radcbm\ at the scale of institutional radiology archives.

On MIMIC-CXR and CheXpert, \radcbm\ matches the classification performance of strong black-box baselines while improving concept AUC and reducing implausible activations compared to flat CBMs.
% with non-hierarchical concept layers.
Automated annotations cover the long tail of radiographic findings without human curation, and the hierarchical architecture exposes region-aware rationales whose counterfactual edits faithfully track the learned decision boundary.

The contributions of this work are threefold:
\begin{itemize}
    \item We introduce \radcbm, a hierarchical concept bottleneck architecture for chest X-rays that organizes concepts by anatomical region, gates region-specific findings through region abnormality scores, and maps the resulting concept vector to diagnostic labels through a linear head, yielding region-aware, concept-level explanations and supporting direct concept interventions.
    \item We present a framework for constructing ontology-grounded concept banks from radiology reports, leveraging RadGraph and the UMLS (including SNOMED CT) to capture finding presence, assertion status, and anatomical localization without manual per-image concept annotation.
    \item We provide an empirical analysis of concept-based representations for chest radiograph classification on MIMIC-CXR and CheXpert, examining how concept vocabulary size, pruning strategies, and supervision density affect downstream performance and interpretability. We also characterize where ontology-derived supervision offers transparency at acceptable cost and where it imposes genuine limitations.
\end{itemize}

The remainder of this paper is organized as follows. Section~\ref{sec:related_work} situates our work within related efforts in chest radiograph analysis, concept-based modeling, and clinical natural language processing. Section X describes the concept extraction pipeline, from report preprocessing through entity linking to concept bank construction, and details the model architectures and training procedures for both concept prediction and downstream classification. Section X presents experimental results on large-scale chest radiograph datasets. Section X discusses limitations, clinical implications, and directions for future work.
