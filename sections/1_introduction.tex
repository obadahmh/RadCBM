Deep models for chest X-ray interpretation now match radiologist-level accuracy on several benchmarks, yet their predictions remain difficult to trust.
Clinicians want to know which anatomical regions appear abnormal and which findings support a label, but conventional convolutional neural networks expose only an opaque probability.

Concept bottleneck models (CBMs) offer a direct path to interpretable reasoning by forcing predictions to flow through human-readable concepts \cite{koh2020concept}, but existing CBMs depend on manual concept annotations that do not scale to hundreds of thousands of studies and dozens of findings.
Recent work revisits CBMs with stronger supervision and probing: Visual TCAV tightens concept attribution by grounding concept directions in intermediate features \cite{desantis2024visualtcav}; Kim et al.\ study robustness under concept shift and label noise \cite{kim2024shiftcbm}; and sparse autoencoders carve pretrained representations into disentangled concepts \cite{rao2024discover}.
Parallel efforts push toward robustness and openness: ConceptGuard enforces distributional stability of concept heads \cite{zhang2024conceptguard}, sparsity-guided bottlenecks improve sample efficiency \cite{liu2024sparsecbm}, open-vocabulary CBMs score unseen concepts using vision--language encoders \cite{yang2024openvocabcbm}, and counterfactual concept editing probes faithfulness \cite{park2024counterfactualcbm}.
These advances improve faithfulness, but most still assume curated vocabularies or boutique annotation pipelines.

Radiology reports already describe the concepts CBMs need.
They indicate which regions are abnormal, which findings are present or absent, and where they are located.
Report-driven explainability is moving toward tighter alignment of text and image.
CheXagent \cite{tu2024chexagent} parses reports and grounds them in image regions to deliver long-form rationales, while pipelines like RadGraph \cite{jain2021radgraph} and CheXbert-style labelers \cite{smit2020chexbert} convert free text into structured observations.
Recent chest X-ray pipelines pretrain joint vision--language encoders to improve alignment and localization \cite{xu2024cxrvlm,lin2024radclip}, denoise report-derived labels before supervision \cite{chen2024denoise}, impose structured-report consistency during training \cite{huang2024structuredreports}, generate hierarchical rationales that mirror report structure \cite{he2025reasoning}, and apply prompt-based adapters for controllable explanations on frozen encoders \cite{gupta2024promptcxr}.
These systems demonstrate that reports provide abundant supervision, but they typically supervise black-box classifiers or post-hoc rationalizers rather than constraining the core decision path.
The remaining gap is to turn report-derived concepts into the actual bottleneck through which predictions must pass, preserving clinical structure without sacrificing accuracy.

We introduce \radcbm, a hierarchical CBM trained end-to-end from paired chest X-rays and reports without manual concept labels.
RadCBM uses RadGraph \cite{jain2021radgraph} to extract observation and anatomy entities, normalizes them to RadLex/UMLS, filters them to clinically meaningful semantic types, and groups them into an anatomy-first hierarchy.
Region abnormality gates region-specific finding predictions, and a linear label head maps these gated concepts to diagnostic labels.
The model therefore mirrors how radiologists read: region assessment first, findings second, diagnosis last.

Our main contributions are:
\begin{itemize}
    \item An automated concept annotation pipeline that converts free-text radiology reports into soft concept targets aligned with standardized vocabularies, eliminating manual labeling.
    \item A hierarchical CBM with multiplicative gating between region abnormality and findings, yielding clinically plausible concepts and faithful linear label explanations.
    \item Empirical evidence on MIMIC-CXR and CheXpert that automated concepts retain coverage, the hierarchy improves concept fidelity over flat CBMs, and classification accuracy matches a black-box baseline while exposing actionable explanations.
\end{itemize}

The remainder of the paper reviews related interpretability and report-mining work, details the RadCBM pipeline, and presents quantitative and qualitative evaluations.
