Deep models for chest X-ray interpretation now match radiologist-level accuracy on CheXpert and MIMIC-CXR \cite{rajpurkar2017chexnet,irvin2019chexpert}.
They are increasingly deployed for triage, second reading, and quality assurance, yet their decision processes remain opaque \cite{annarumma2019triage}.
This opacity has concrete clinical consequences, affecting how models are audited, trusted, and integrated into workflow.
When a network predicts ``pneumonia,'' clinicians cannot tell whether it responded to a genuine parenchymal finding or to confounders like support devices and acquisition artifacts.
Failure analysis teams lack actionable signals for understanding systematic errors.
Empirical work shows that chest X-ray models can generalize unpredictably across hospitals \cite{zech2018variable}, while clinician interviews and position papers argue that high-stakes medical decisions require transparent, interpretable models rather than unexplained black boxes \cite{tonekaboni2019clinicians,rudin2019stop}.
In this work, our goal is not to eliminate distribution shift, but to make such failures more diagnosable by exposing region- and concept-level reasoning; we focus on in-domain performance on MIMIC-CXR and CheXpert.
Radiologists, by contrast, describe findings as localized concepts; for example, ``left lower lobe opacity with no pleural effusion,'' tying observations to anatomy and differential diagnoses.
A trustworthy system should expose this intermediate reasoning, not just an uninterpretable probability \cite{langlotz2006radlex,jain2021radgraph}.
Together, these results and perspectives motivate explainability methods for chest X-ray classifiers that make explicit which image regions and clinical concepts drive each decision \cite{rajpurkar2017chexnet,irvin2019chexpert,annarumma2019triage,zech2018variable}.

Post-hoc explanation methods attempt to alleviate this opacity by highlighting pixels that most influence a model's output or by fitting local surrogates around each prediction.
Saliency and gradient-based techniques such as Grad-CAM and integrated gradients visualize where a network is ``looking'' \cite{selvaraju2017gradcam,sundararajan2017axiomatic}, while perturbation-based approaches such as LIME fit simpler models that approximate the decision boundary near a given image \cite{ribeiro2016lime}.
In chest X-ray applications these tools can help detect obvious failure modes (for example, models that rely on laterality markers or support devices) but they typically operate at the level of diffuse blobs rather than named clinical findings, and their outputs can be unstable across small input or parameter changes.
Moreover, post-hoc tools treat the network as a fixed black box and do not constrain its internal representation to align with named radiologic concepts, which limits how much trust and control clinicians can derive from them in practice \cite{selvaraju2017gradcam,sundararajan2017axiomatic,ribeiro2016lime,zech2018variable}.

Concept-based explanations aim to bridge this gap by expressing predictions in terms of high-level attributes rather than raw pixels, an idea instantiated by TCAV-style sensitivity analyses and concept bottleneck models \cite{kim2018tcav,koh2020concept,desantis2024visualtcav,kim2024shiftcbm}.
TCAV-style approaches test the sensitivity of a model to user-defined concept vectors, and concept bottleneck models (CBMs) directly embed this philosophy by routing predictions through a layer of human-interpretable concepts before computing labels.
When concept activations are accurate, CBMs provide faithful, editable explanations because each class logit decomposes into contributions from named findings, and clinicians can intervene by editing concepts rather than raw pixels.
However, classical CBMs assume that each training image comes with dense concept annotations, and the annotation burden grows with the product of dataset size and number of concepts.
Typical chest X-ray datasets contain more than $10^5$ studies and $50$ to $200$ relevant findings, so naively collecting expert concept labels would require millions of radiologist judgments and careful quality control \cite{johnson2019mimic,irvin2019chexpert}.
Recent CBM work explores better attribution \cite{desantis2024visualtcav}, robustness under concept shift \cite{kim2024shiftcbm}, and sparse autoencoders that carve pretrained representations into reusable features \cite{rao2024discover}, but these directions still depend on curated concept sets or small-scale supervision, and they rarely tackle the long tail of infrequent but clinically important findings that appear in real-world radiology practice.

A large fraction of chest X-ray reports describes the very region-specific findings that CBMs aim to predict, but reports also contain clinical impressions and non-visual statements (for example, indications and management plans) that cannot be inferred from images \cite{jain2021radgraph,smit2020chexbert}.
A single sentence such as ``patchy opacity in the left lower lobe consistent with pneumonia. No pleural effusion. Heart size is normal.'' implicitly labels the presence of lung opacity and pneumonia and the absence of effusion and cardiomegaly for that study, while leaving other structures unspecified.
If these signals can be extracted, normalized to standardized vocabularies, and grouped by anatomy, they could supervise CBMs at corpus scale without manual per-concept labeling, provided that non-visual or temporal concepts are filtered out and uncertainty in reports is handled explicitly.
The core challenge becomes turning noisy report parses into an ontology-grounded, hierarchical concept vocabulary that supports region-aware explanations.
This vocabulary should keep only visual, image-evident findings while maintaining high coverage of common and rare pathologies.

Existing chest X-ray explainability pipelines already tap into reports but stop short of making concepts the decision bottleneck.
CheXagent produces long-form rationales by aligning report sentences with image regions \cite{tu2024chexagent}, and tools such as RadGraph \cite{jain2021radgraph} and CheXbert \cite{smit2020chexbert} extract structured observation-anatomy pairs with uncertainty tags from free text.
These systems demonstrate that reports provide rich supervisory signal, yet they mainly supervise black-box classifiers or post-hoc rationalizers whose internal pathways remain opaque and are not constrained to use only visual, region-grounded concepts, so their explanations can disagree with the actual features that drive the prediction.
Some chest X-ray classifiers incorporate label hierarchies or anatomy-aware heads, but they typically use these structures as regularizers on latent features: predictions are still made directly from opaque embeddings rather than explicit concepts.
In contrast, \radcbm\ constrains all labels to be linear functions of gated region and finding activations, so explanations are faithful to the decision path and directly editable.

Compared to prior chest X-ray pipelines that use reports only to derive global labels or generate rationales, \radcbm\ turns report-derived observation-region pairs into the primary representational bottleneck: we automatically construct a large, ontology-anchored vocabulary of region-specific findings and require all label predictions to flow through this vocabulary.
This goes beyond simply training a multi-task model on RadGraph/CheXbert labels or attaching a generic CBM on top of a small curated concept set, because every prediction decomposes into contributions from clinically meaningful, region-specific concepts that can be directly inspected and edited.

We introduce \radcbm, a hierarchical CBM trained end-to-end from paired chest X-rays and reports without manual concept labels.
\radcbm\ can be viewed as instantiating the CBM paradigm in the chest X-ray setting while addressing the scalability limitations of prior concept-based approaches, by learning a clinically grounded concept space directly from reports instead of relying on hand-designed concept sets or small-scale annotations \cite{koh2020concept,kim2018tcav,desantis2024visualtcav,kim2024shiftcbm}.
\radcbm\ uses RadGraph to extract observation-anatomy pairs, normalizes them to RadLex/UMLS concepts, anchoring the vocabulary in established clinical ontologies, filters them to chest-focused semantic types and to concepts that are visually testable, and organizes the resulting vocabulary into anatomical regions with associated findings.
A two-level predictor maps images to region abnormality scores and region-specific finding probabilities, applies multiplicative gating so that findings activate only when their region is abnormal, and feeds the gated concept vector to a transparent linear label head.
Fixing the hierarchy to anatomy $\rightarrow$ finding mirrors radiologist workflows and avoids learned groupings that may be difficult to interpret, while the linear head keeps concept contributions directly readable and editable and is designed to reduce the influence of non-visual report artifacts (for example, mentions of lines or tubes) on predictions.
This architecture mirrors radiologists' workflow (assess regions, describe findings, decide labels) and produces explanations that can be audited or edited at both region and concept levels.

Taken together, these design choices move chest X-ray explainability beyond generic saliency maps and small-scale concept probes toward a clinically grounded, editable reasoning process that mirrors radiologist workflows and scales to large, heterogeneous datasets \cite{selvaraju2017gradcam,sundararajan2017axiomatic,ribeiro2016lime,rajpurkar2017chexnet,irvin2019chexpert,annarumma2019triage,zech2018variable,koh2020concept,kim2018tcav,desantis2024visualtcav,kim2024shiftcbm}.

Our main contributions are:
\begin{itemize}
    \item An automated concept annotation pipeline that converts free-text radiology reports into soft, ontology-grounded concept targets aligned with standardized vocabularies, eliminating manual per-concept labeling while retaining a broad, clinically meaningful vocabulary of visual findings and anatomical regions.
    \item A hierarchical CBM with multiplicative gating between region abnormality and findings, enforcing clinical consistency (findings fire only in abnormal regions) and enabling region-first, finding-level explanations and counterfactual concept interventions.
    \item An empirical study on MIMIC-CXR and CheXpert comparing \radcbm\ to black-box CNNs, flat and post-hoc CBMs, and CheXpert-style concept sets, evaluating label performance (AUC/F1) on CheXpert findings, concept fidelity and intervention faithfulness, and plausibility and coverage of region-level explanations; automated concepts retain broad coverage, the hierarchy improves concept fidelity and reduces implausible activations (for example, subtle pneumothorax or early pneumomediastinum), and classification performance remains comparable to a black-box CNN while exposing actionable, region-aware explanations.
\end{itemize}
