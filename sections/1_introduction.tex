Deep models for chest X-ray interpretation now match radiologist-level accuracy on several benchmarks, yet their predictions remain difficult to trust.
Clinicians want to know which anatomical regions appear abnormal and which findings support a label, but conventional convolutional neural networks expose only an opaque probability.
Concept bottleneck models (CBMs) offer a direct path to interpretable reasoning by forcing predictions to flow through human-readable concepts \cite{koh2020concept}, but existing CBMs depend on manual concept annotations that do not scale to hundreds of thousands of studies and dozens of findings.

Radiology reports already describe the concepts CBMs need.
They indicate which regions are abnormal, which findings are present or absent, and where they are located.
If those reports can be converted into reliable training targets, interpretability-by-design no longer requires expensive human labeling.
The challenge is to extract clinically grounded concepts automatically and to structure them so that explanations remain faithful to radiologist workflows.

We introduce \radcbm, a hierarchical CBM trained end-to-end from paired chest X-rays and reports without manual concept labels.
RadCBM uses RadGraph \cite{jain2021radgraph} to extract observation and anatomy entities, normalizes them to RadLex/UMLS, filters them to clinically meaningful semantic types, and groups them into an anatomy-first hierarchy.
Region abnormality gates region-specific finding predictions, and a linear label head maps these gated concepts to diagnostic labels.
The model therefore mirrors how radiologists read: region assessment first, findings second, diagnosis last.

Our main contributions are:
\begin{itemize}
    \item An automated concept annotation pipeline that converts free-text radiology reports into soft concept targets aligned with standardized vocabularies, eliminating manual labeling.
    \item A hierarchical CBM with multiplicative gating between region abnormality and findings, yielding clinically plausible concepts and faithful linear label explanations.
    \item Empirical evidence on MIMIC-CXR and CheXpert that automated concepts retain coverage, the hierarchy improves concept fidelity over flat CBMs, and classification accuracy matches a black-box baseline while exposing actionable explanations.
\end{itemize}

The remainder of the paper reviews related interpretability and report-mining work, details the RadCBM pipeline, and presents quantitative and qualitative evaluations.
