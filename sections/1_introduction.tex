Chest radiography remains the most frequently performed imaging examination worldwide, with hundreds of millions of studies acquired annually~\cite{raoof2012interpretation}. Interpreting these images is high-stakes: a missed pneumothorax, an overlooked nodule, or a mischaracterized cardiac silhouette can alter the trajectory of patient care~\cite{bruno2015understanding}. Because the sheer volume of studies strains radiology workflows, diagnostic errors (while individually rare) accumulate into a substantial burden when multiplied across populations~\cite{brady2017error,donald2014common}. The promise of computational assistance is therefore not merely academic. Systems that can reliably flag abnormalities, prioritize urgent cases, or provide differential considerations address a genuine clinical need~\cite{litjens2017survey}.

Deep learning has delivered remarkable progress toward this goal. Convolutional and transformer-based architectures now match or exceed physician-level performance on curated benchmarks for thoracic pathology detection~\cite{rajpurkar2017chexnet,irvin2019chexpert,singh2024efficient,shamshad2023transformers}. These results, however, have not translated proportionally into clinical deployment~\cite{kelly2019key,nagendran2020artificial}. Part of this gap reflects concerns about robustness and generalization: models that perform well on internal test sets can fail when applied to external hospitals, sometimes because they exploit institution-specific artifacts rather than disease-related signal~\cite{zech2018confounding}. Reasons also include regulatory, infrastructural, and cultural barriers~\cite{kelly2019key}.

A recurrent critique in high-stakes clinical machine learning is that black-box predictions lack inspectable reasoning~\cite{rudin2019stop}. A model may assert ``cardiomegaly'' with high confidence, but it cannot articulate \emph{why} or point to the cardiothoracic ratio it implicitly computed. It cannot translate that confidence into the kinds of criteria clinicians expect, such as measured ratios or other anatomically grounded evidence. This is not mere aesthetic preference for explanation. Radiologists think in concepts such as consolidation, air bronchograms, Kerley lines, and costophrenic blunting, and a system that cannot speak this language offers predictions without a basis for trust or correction~\cite{reyes2020interpretability}.

Concept-based models, often instantiated as Concept Bottleneck Models (CBMs), offer an architectural response to this limitation~\cite{koh2020concept}. Instead of mapping pixels directly to diagnostic labels, they introduce an intermediate representation of human-interpretable attributes. The model first predicts whether specific concepts are present (anatomical structures, radiographic findings, device positions) and then uses those concepts to produce diagnostic outputs. Explanations are thus part of the forward pass rather than added post hoc through saliency methods~\cite{selvaraju2017gradcam}. When a CBM predicts pulmonary edema, we can inspect whether it detected cardiomegaly, vascular redistribution, or interstitial opacities and check that this reasoning aligns with clinical knowledge.

This interpretability, however, comes at a cost that has limited practical adoption: concept-based models require concepts. Specifically, they require a predefined vocabulary of clinically meaningful attributes and, more demandingly, supervisory signal indicating which concepts are present in which images. Manual annotation at this granularity is expensive, time-consuming, and difficult to scale~\cite{willemink2020preparing}. A single chest radiograph might exhibit dozens of relevant findings across multiple anatomical regions, each requiring expert assessment. Curated ontologies such as SNOMED CT standardize terminology and relations~\cite{donnelly2006snomed}, but they do not provide image-grounded labels, such as presence, laterality, or anatomical site, for individual radiographs, so the core supervision requirement remains unchanged. The gap between the conceptual richness that would make these models clinically useful and the annotation budgets that real projects can sustain has constrained concept-based approaches to modest scales or narrow concept sets~\cite{oikarinen2023labelfree}.

Recent attempts to apply concept-based models to medical imaging have pursued two directions, neither fully satisfactory for chest radiography. The first generates concept vocabularies from large language models: one approach prompts GPT-4 to enumerate radiographic findings, then projects CLIP embeddings onto these concepts~\cite{yan2023robust,kim2023visualconceptfiltering}. While this eliminates manual annotation, LLM-generated concepts lack grounding in clinical ontologies, may include findings that are not visually testable from a frontal radiograph, and inherit the hallucination tendencies of their source models. The second integrates clinical knowledge by guiding models to prioritize clinically important concepts through alignment losses~\cite{pang2024integrating,yang2024textbook}. However, such approaches require expert-provided importance rankings for each concept and have not been demonstrated to scale beyond small concept sets; enumeration-based importance weighting becomes intractable when dozens or hundreds of concepts are involved, as in chest radiography.

We pursue a different direction: repurposing existing clinical NLP tools as sources of concept supervision. Tools such as RadGraph~\cite{jain2021radgraph} parse reports into entity--relation graphs and have become standard for evaluating report generation quality via RadGraph F1 scores, but, to our knowledge, have not been used to \emph{supervise} concept bottleneck models. Their structured outputs remain confined to evaluation metrics rather than serving as trainable concept targets. Meanwhile, routine radiology reports already encode rich conceptual supervision: by the time a radiologist documents ``right basilar pneumonia,'' they have localized disease, described its radiographic pattern, and linked observations to a diagnostic impression. This information is recorded in natural language rather than structured labels, but it is expert-generated, temporally aligned with the image, and available at scale in virtually every institution with an electronic health record~\cite{johnson2019mimiccxr}. The challenge is to turn this free text into supervision suitable for training concept-based vision models.

Transforming free-text reports into structured concept representations is not straightforward. Radiology language is dense with abbreviations, implicit negations, and context-dependent qualifications~\cite{denny2009extracting}. A finding may be ``present,'' ``absent,'' ``unchanged,'' or ``cannot be excluded,'' distinctions that matter clinically and must be preserved in any derived supervision~\cite{chapman2001negex,smit2020chexbert}. Linking extracted mentions to standardized terminologies introduces additional complexity: the same concept may be expressed in myriad surface forms, and disambiguation requires domain-specific knowledge. Recent advances in clinical natural language processing and biomedical entity linking~\cite{liu2021sapbert,jain2021radgraph}, together with resources such as the Unified Medical Language System (UMLS)~\cite{bodenreider2004umls}, make such extraction increasingly tractable. However, these tools have rarely been combined into pipelines that produce \emph{trainable} concept banks with assertion status, anatomical context, and ontological grounding.

This work addresses the gap between the latent supervision encoded in radiology reports and the structured representations that concept-based vision models require. Prior efforts have tackled adjacent problems: extracting findings from clinical text~\cite{peng2018negbio,irvin2019chexpert}, linking medical entities to ontologies such as UMLS~\cite{bodenreider2004umls}, and training interpretable classifiers on manually curated concept sets~\cite{koh2020concept}. These efforts, however, stop short of turning large report corpora into trainable, ontology-grounded concept banks and pairing them with hierarchical CBMs for chest radiography. Unlike systems that use reports primarily to derive noisy image-level labels for black-box classifiers or that restrict CBMs to small, hand-designed concept sets, we convert routine report corpora into ontology-grounded concept banks and use them to supervise \radcbm\ at the scale of institutional radiology archives. In contrast to alignment-loss approaches that require per-concept importance annotations~\cite{pang2024integrating}, \radcbm\ encodes clinical knowledge structurally: ontology grounding via UMLS provides semantic standardization, and the hierarchical architecture with multiplicative gating enforces anatomy-first reasoning without additional human input.

On MIMIC-CXR and CheXpert, \radcbm\ matches the classification performance of strong black-box baselines while improving concept AUC and reducing implausible activations compared to flat CBMs.
% with non-hierarchical concept layers.
Automated annotations cover the long tail of radiographic findings without human curation, and the hierarchical architecture exposes region-aware rationales whose counterfactual edits faithfully track the learned decision boundary.

The contributions of this work are threefold:
\begin{itemize}
    \item We introduce \radcbm, the first hierarchical concept bottleneck architecture for chest radiography that organizes concepts by anatomical region, derives region abnormality targets by pooling RadGraph-extracted concept locations, and gates region-specific findings through those derived region scores (no separate region annotations), while constraining label predictions to linear functions of gated concepts. This design enforces clinical consistency (lung findings cannot fire when lungs are predicted normal) and produces explanations aligned with radiologist workflows.
    \item We present a framework that repurposes RadGraph, previously used only for report generation evaluation, as a source of trainable concept supervision. By linking extracted mentions to SNOMED CT via the UMLS and preserving assertion status, we construct ontology-grounded concept banks at scale without manual per-image annotation, covering hundreds of region-specific findings beyond the 14-class vocabularies typical of prior work.
    \item We provide empirical analysis on MIMIC-CXR and CheXpert demonstrating that \radcbm\ matches black-box classification accuracy while improving concept AUC over flat CBMs, reducing implausible activations through gating, and enabling faithful concept interventions whose effects reliably track the learned decision boundary.
\end{itemize}

The remainder of this paper is organized as follows. Section~\ref{sec:related_work} situates our work within related efforts in chest radiograph analysis, concept-based modeling, and clinical natural language processing. Section X describes the concept extraction pipeline, from report preprocessing through entity linking to concept bank construction, and details the model architectures and training procedures for both concept prediction and downstream classification. Section X presents experimental results on large-scale chest radiograph datasets. Section X discusses limitations, clinical implications, and directions for future work.
