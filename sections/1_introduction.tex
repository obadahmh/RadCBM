Deep models for chest X-ray interpretation now match radiologist-level accuracy on CheXpert and MIMIC-CXR \cite{rajpurkar2017chexnet,irvin2019chexpert}.
They are increasingly deployed for triage, second reading, and quality assurance, yet their decision processes remain opaque \cite{annarumma2019triage}.
This opacity is not merely aesthetic.
When a network predicts ``pneumonia,'' clinicians cannot tell whether it responded to a genuine parenchymal finding or to confounders like support devices and acquisition artifacts.
Failure analysis teams lack actionable signals for understanding systematic errors.
Empirical work shows that chest X-ray models can generalize unpredictably across hospitals \cite{zech2018variable}, while clinician interviews and position papers argue that high-stakes medical decisions require transparent, interpretable models rather than unexplained black boxes \cite{tonekaboni2019clinicians,rudin2019stop}.
Radiologists, by contrast, describe findings as localized concepts—for example, ``left lower lobe opacity with no pleural effusion''—tying observations to anatomy and differential diagnoses.
A trustworthy system should expose this intermediate reasoning, not just an uninterpretable probability \cite{langlotz2006radlex,jain2021radgraph}.

Post-hoc explanation methods attempt to alleviate this opacity by highlighting pixels that most influence a model's output or by fitting local surrogates around each prediction.
Saliency and gradient-based techniques such as Grad-CAM and integrated gradients visualize where a network is ``looking'' \cite{selvaraju2017gradcam,sundararajan2017axiomatic}, while perturbation-based approaches such as LIME fit simpler models that approximate the decision boundary near a given image \cite{ribeiro2016lime}.
In chest X-ray applications these tools can help detect obvious failure modes---for example, models that rely on laterality markers or support devices---but they typically operate at the level of diffuse blobs rather than named clinical findings, and their outputs can be unstable across small input or parameter changes.

Concept-based explanations move one step closer to radiologist reasoning by expressing predictions in terms of high-level attributes rather than raw pixels.
TCAV-style approaches test the sensitivity of a model to user-defined concept vectors \cite{kim2018tcav,desantis2024visualtcav}, and concept bottleneck models (CBMs) directly embed this philosophy by routing predictions through a layer of human-interpretable concepts before computing labels \cite{koh2020concept,kim2024shiftcbm}.
When concept activations are accurate, CBMs provide faithful, editable explanations because each class logit decomposes into contributions from named findings, and clinicians can intervene by editing concepts rather than raw pixels.
However, classical CBMs assume that each training image comes with dense concept annotations, and the annotation burden grows with the product of dataset size and number of concepts.
Typical chest X-ray datasets contain more than $10^5$ studies and $50$--$200$ relevant findings, so naively collecting expert concept labels would require millions of radiologist judgments and careful quality control \cite{johnson2019mimic,irvin2019chexpert}.
Recent CBM work explores better attribution \cite{desantis2024visualtcav}, robustness under concept shift \cite{kim2024shiftcbm}, and sparse autoencoders that carve pretrained representations into reusable features \cite{rao2024discover}, but these directions still depend on curated concept sets or small-scale supervision, and they rarely tackle the long tail of infrequent but clinically important findings that appear in real-world radiology practice.

A large fraction of chest X-ray reports describes the very region-specific findings that CBMs aim to predict, but reports also contain clinical impressions and non-visual statements (for example, indications and management plans) that cannot be inferred from images \cite{jain2021radgraph,smit2020chexbert}.
A single sentence such as ``patchy opacity in the left lower lobe consistent with pneumonia. No pleural effusion. Heart size is normal.'' implicitly labels the presence of lung opacity and pneumonia and the absence of effusion and cardiomegaly for that study, while leaving other structures unspecified.
If these signals can be extracted, normalized to standardized vocabularies, and grouped by anatomy, they could supervise CBMs at corpus scale without manual per-concept labeling, provided that non-visual or temporal concepts are filtered out and uncertainty in reports is handled explicitly.
The core challenge becomes turning noisy report parses into an ontology-grounded, hierarchical concept vocabulary that supports region-aware explanations while keeping only visual, image-evident findings and maintaining high coverage of common and rare pathologies.

Existing chest X-ray explainability pipelines already tap into reports but stop short of making concepts the decision bottleneck.
CheXagent produces long-form rationales by aligning report sentences with image regions \cite{tu2024chexagent}, and tools such as RadGraph \cite{jain2021radgraph} and CheXbert \cite{smit2020chexbert} extract structured observation--anatomy pairs with uncertainty tags from free text.
These systems demonstrate that reports provide rich supervisory signal, yet they mainly supervise black-box classifiers or post-hoc rationalizers whose internal pathways remain opaque and are not constrained to use only visual, region-grounded concepts, so their explanations can disagree with the actual features that drive the prediction.

We introduce \radcbm, a hierarchical CBM trained end-to-end from paired chest X-rays and reports without manual concept labels.
\radcbm\ uses RadGraph to extract observation--anatomy pairs, normalizes them to RadLex/UMLS concepts so the vocabulary inherits a clinically grounded ontology, filters them to chest-focused semantic types and to concepts that are visually testable, and organizes the resulting vocabulary into anatomical regions with associated findings.
A two-level predictor maps images to region abnormality scores and region-specific finding probabilities, applies multiplicative gating so that findings activate only when their region is abnormal, and feeds the gated concept vector to a transparent linear label head.
Fixing the hierarchy to anatomy $\rightarrow$ finding mirrors radiologist workflows and avoids learned groupings that may be difficult to interpret, while the linear head keeps concept contributions directly readable and editable and prevents non-visual report artifacts (for example, mentions of lines or tubes) from driving predictions.
This architecture mirrors radiologists' workflow---assess regions, describe findings, decide labels---and produces explanations that can be audited or edited at both region and concept levels.

Our main contributions are:
\begin{itemize}
    \item An automated concept annotation pipeline that converts free-text radiology reports into soft, ontology-grounded concept targets aligned with standardized vocabularies, eliminating manual per-concept labeling while retaining a broad, clinically meaningful vocabulary of visual findings and anatomical regions.
    \item A hierarchical CBM with multiplicative gating between region abnormality and findings, enforcing clinical consistency (findings fire only in abnormal regions) and enabling region-first, finding-level explanations and counterfactual concept interventions.
    \item An empirical study on MIMIC-CXR and CheXpert comparing \radcbm\ to black-box CNNs, flat CBMs, and CheXpert-style concept sets, showing that automated concepts retain broad coverage, the hierarchy improves concept fidelity and reduces implausible activations, and classification accuracy matches a black-box CNN while exposing actionable, region-aware explanations.
\end{itemize}
