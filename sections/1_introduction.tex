Deep models for chest X-ray interpretation now match radiologist-level accuracy on large public benchmarks, yet their decision processes remain opaque.
Opacity is not merely aesthetic: clinicians cannot check whether a model focused on the right anatomy, failure analysis lacks actionable signals, and regulators expect explanations before approving clinical software \cite{zech2018variable}.
Radiologists describe their findings as localized concepts (for example, ``left lower lobe opacity with no pleural effusion''), so a trustworthy system should expose the same intermediate reasoning rather than returning only a probability.

Concept bottleneck models (CBMs) directly embed this philosophy by routing predictions through a layer of human-interpretable concepts before computing labels \cite{koh2020concept}.
When concept activations are accurate, CBMs provide faithful, editable explanations because each class logit decomposes into contributions from named findings.
However, classical CBMs assume that each training image comes with dense concept annotations, and the annotation burden grows with the product of dataset size and number of concepts.
Typical chest X-ray datasets contain more than $10^5$ studies and $50$--$200$ relevant findings, so naively collecting expert concept labels would require millions of radiologist judgments.
Recent CBM work explores better attribution \cite{desantis2024visualtcav}, robustness under concept shift \cite{kim2024shiftcbm}, and sparse autoencoders that carve pretrained representations into reusable features \cite{rao2024discover}, but these directions still depend on curated concept sets or small-scale supervision.

A large fraction of chest X-ray reports describes the very region-specific findings that CBMs aim to predict, but reports also contain clinical impressions and non-visual statements that cannot be inferred from images.
A single sentence such as ``patchy opacity in the left lower lobe consistent with pneumonia. No pleural effusion. Heart size is normal.'' implicitly labels the presence of lung opacity and pneumonia and the absence of effusion and cardiomegaly.
If these signals can be extracted, normalized to standardized vocabularies, and grouped by anatomy, they could supervise CBMs at corpus scale without manual per-concept labeling.
The core challenge becomes turning noisy report parses into an ontology-grounded, hierarchical concept vocabulary that supports region-aware explanations.

Existing chest X-ray explainability pipelines already tap into reports but stop short of making concepts the decision bottleneck.
CheXagent produces long-form rationales by aligning report sentences with image regions \cite{tu2024chexagent}, and tools such as RadGraph \cite{jain2021radgraph} and CheXbert \cite{smit2020chexbert} extract structured observation--anatomy pairs with uncertainty tags.
These systems demonstrate that reports provide rich supervisory signal, yet they mainly supervise black-box classifiers or post-hoc rationalizers whose internal pathways remain opaque.

We introduce \radcbm, a hierarchical CBM trained end-to-end from paired chest X-rays and reports without manual concept labels.
\radcbm\ uses RadGraph to extract observation--anatomy pairs, normalizes them to RadLex/UMLS concepts so the vocabulary inherits a clinically grounded ontology, filters them to chest-focused semantic types, and organizes the resulting vocabulary into anatomical regions with associated findings.
A two-level predictor maps images to region abnormality scores and region-specific finding probabilities, applies multiplicative gating so that findings activate only when their region is abnormal, and feeds the gated concept vector to a transparent linear label head.
Fixing the hierarchy to anatomy $\rightarrow$ finding mirrors radiologist workflows and avoids learned groupings that may be difficult to interpret, while the linear head keeps concept contributions directly readable and editable.
This architecture mirrors radiologists' workflow---assess regions, describe findings, decide labels---and produces explanations that can be audited or edited at both region and concept levels.

Our main contributions are:
\begin{itemize}
    \item An automated concept annotation pipeline that converts free-text radiology reports into soft, ontology-grounded concept targets aligned with standardized vocabularies, eliminating manual per-concept labeling.
    \item A hierarchical CBM with multiplicative gating between region abnormality and findings, enforcing clinical consistency and enabling region-first, finding-level explanations.
    \item An empirical study on MIMIC-CXR and CheXpert showing that automated concepts retain broad coverage, the hierarchy improves concept fidelity over flat CBMs and CheXpert-style concept sets, and classification accuracy matches a black-box CNN while exposing actionable concept interventions.
\end{itemize}
