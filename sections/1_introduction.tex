Chest radiography remains the most frequently performed imaging examination worldwide, with hundreds of millions of studies acquired annually~\cite{raoof2012interpretation}. Interpreting these images is high-stakes: a missed pneumothorax, an overlooked nodule, or a mischaracterized cardiac silhouette can alter the trajectory of patient care~\cite{bruno2015understanding}. Because the sheer volume of studies strains radiology workflows, diagnostic errors (while individually rare) accumulate into a substantial burden when multiplied across populations~\cite{brady2017error,donald2014common}. The volume alone creates genuine clinical need for systems that reliably flag abnormalities or prioritize urgent cases~\cite{litjens2017survey}.

Deep learning has delivered remarkable progress toward this goal. Convolutional and transformer-based architectures now match or exceed physician-level performance on curated benchmarks for thoracic pathology detection~\cite{rajpurkar2017chexnet,irvin2019chexpert,singh2024efficient,shamshad2023transformers}. But these results have seen limited clinical deployment~\cite{kelly2019key,nagendran2020artificial}. Part of this gap reflects concerns about robustness. Models that perform well on internal test sets can fail at external hospitals, sometimes because they exploit institution-specific artifacts rather than disease-related signal~\cite{zech2018confounding}. Regulatory, infrastructural, and cultural barriers contribute as well~\cite{kelly2019key}.

A recurrent critique in high-stakes clinical machine learning is that black-box predictions lack inspectable reasoning~\cite{rudin2019stop}. A model may assert ``cardiomegaly'' with high confidence, but cannot articulate why or point to the cardiothoracic ratio it implicitly computed. It cannot translate that confidence into the criteria clinicians expect, such as measured ratios, anatomical landmarks, or radiographic patterns. Radiologists reason in concepts such as consolidation, air bronchograms, Kerley lines, and costophrenic blunting. A system that cannot engage with this vocabulary offers predictions without a basis for trust or correction~\cite{reyes2020interpretability}.

Concept Bottleneck Models (CBMs) offer an architectural response to this limitation~\cite{koh2020concept}. Instead of mapping pixels directly to diagnostic labels, they introduce an intermediate representation of human-interpretable attributes. The model first predicts whether specific concepts are present (anatomical structures, radiographic findings, device positions) and then uses those concepts to produce diagnostic outputs. Explanations become part of the forward pass rather than post-hoc additions through saliency methods~\cite{selvaraju2017gradcam}. When a CBM predicts pulmonary edema, we can inspect whether it detected cardiomegaly, vascular redistribution, or interstitial opacities and verify that this reasoning aligns with clinical knowledge.

This interpretability comes at a cost that has limited practical adoption: concept-based models require concepts. Specifically, they require a predefined vocabulary of clinically meaningful attributes and, more demandingly, supervisory signal indicating which concepts are present in which images. Manual annotation at this granularity is prohibitively expensive and does not scale~\cite{willemink2020preparing,oikarinen2023labelfree}. A single chest radiograph might exhibit dozens of relevant findings across multiple anatomical regions, each requiring expert assessment.

Recent attempts to apply concept-based models to medical imaging have pursued two directions, neither satisfactory for chest radiography. One line of work generates concept vocabularies from large language models, prompting GPT-4 to enumerate radiographic findings and projecting CLIP embeddings onto these concepts~\cite{yan2023robust,kim2023visualconceptfiltering}. This eliminates manual annotation but introduces new problems. LLM-generated concepts lack grounding in clinical ontologies, may include findings not visually testable from a frontal radiograph, and inherit hallucination tendencies from their source models. A second line integrates clinical knowledge by guiding models to prioritize clinically important concepts through alignment losses~\cite{pang2024integrating,yang2024textbook}. These approaches require expert-provided importance rankings for each concept and have not scaled beyond small concept sets.

We take a different approach and repurpose existing clinical NLP tools as sources of concept supervision. Routine radiology reports already encode rich conceptual supervision. When a radiologist documents ``right basilar pneumonia,'' they have localized disease, described its radiographic pattern, and linked observations to a diagnostic impression. Tools such as RadGraph~\cite{jain2021radgraph} can extract this structure by parsing reports into entity-relation graphs and are widely used to evaluate report generation quality via RadGraph F1 scores. To our knowledge, they have not been used to supervise concept bottleneck models. Their structured outputs remain confined to evaluation metrics rather than serving as trainable concept targets. This information is recorded in natural language rather than structured labels, but it is expert-generated, temporally aligned with the image, and available at scale in virtually every institution with an electronic health record~\cite{johnson2019mimiccxr}. The challenge is transforming this free text into supervision suitable for training concept-based vision models.

This transformation is challenging. Radiology language is dense with abbreviations, implicit negations, and context-dependent qualifications~\cite{denny2009extracting}. A finding may be ``present,'' ``absent,'' ``unchanged,'' or ``cannot be excluded.'' These distinctions matter clinically and must be preserved in any derived supervision~\cite{chapman2001negex,smit2020chexbert}. Moreover, unmentioned findings are not necessarily absent; radiologists document only what they deem clinically relevant. Linking extracted mentions to standardized terminologies introduces additional complexity, since the same concept may appear in many surface forms and disambiguation requires domain-specific knowledge. Recent advances in clinical natural language processing and biomedical entity linking~\cite{liu2021sapbert,jain2021radgraph}, together with resources such as the Unified Medical Language System (UMLS)~\cite{bodenreider2004umls}, now achieve high accuracy on radiology text. But these tools have rarely been combined into pipelines that produce trainable concept banks with assertion status, anatomical context, and ontological grounding.

Prior work has extracted findings from clinical text~\cite{peng2018negbio,irvin2019chexpert}, linked medical entities to ontologies such as UMLS~\cite{bodenreider2004umls}, and trained interpretable classifiers on manually curated concept sets~\cite{koh2020concept}. Our approach differs from systems that use reports primarily to derive noisy image-level labels for black-box classifiers, and from CBMs restricted to small, hand-designed concept sets. We convert routine report corpora into ontology-grounded concept banks and use them to supervise \radcbm\ at the scale of institutional radiology archives. Unlike alignment-loss approaches that require per-concept importance annotations~\cite{pang2024integrating}, \radcbm\ encodes clinical knowledge structurally. Ontology grounding via UMLS provides semantic standardization, and hierarchical architecture with multiplicative gating enforces anatomy-first reasoning without additional human input.

On MIMIC-CXR and CheXpert, \radcbm\ matches the classification performance of strong black-box baselines while improving concept AUC and reducing implausible activations compared to flat CBMs. Automated annotations cover the long tail of radiographic findings without human curation, and the hierarchical architecture exposes region-aware rationales whose counterfactual edits faithfully track the learned decision boundary.

Our contributions are as follows:
\begin{itemize}
    \item We introduce \radcbm, a hierarchical concept bottleneck architecture that organizes concepts by anatomical region and gates region-specific findings through learned region abnormality scores derived from RadGraph-extracted concept locations. Label predictions are linear functions of gated concepts, enforcing clinical consistency (lung findings cannot fire when lungs are predicted normal) without separate region annotations.
    
    \item We present a framework that repurposes RadGraph as a source of trainable concept supervision rather than solely an evaluation metric. By linking extracted mentions to SNOMED CT via the UMLS and preserving assertion status, we construct ontology-grounded concept banks at scale without manual per-image annotation, covering hundreds of region-specific findings beyond the 14-class vocabularies typical of prior work.
    
    \item We provide empirical analysis on MIMIC-CXR and CheXpert demonstrating that \radcbm\ matches black-box classification accuracy while improving concept AUC over flat CBMs, reducing implausible activations through gating, and enabling faithful concept interventions whose effects reliably track the learned decision boundary.
\end{itemize}

% The remainder of this paper is organized as follows. Section~\ref{sec:related_work} situates our work within related efforts in chest radiograph analysis, concept-based modeling, and clinical natural language processing. Section~X describes the concept extraction pipeline, from report preprocessing through entity linking to concept bank construction, and details model architectures and training procedures for both concept prediction and downstream classification. Section~X presents experimental results on large-scale chest radiograph datasets. Section~X discusses limitations, clinical implications, and directions for future work.
